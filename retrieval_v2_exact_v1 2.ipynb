{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "OPENAI_MODEL=\"gpt-3.5-turbo\"\n",
    "OPENAI_MULTI_MODEL=\"gpt-4o\"\n",
    "OPENAI_MODEL_4O=\"gpt-4o-mini\"\n",
    "NO_OF_CLOSEST_NODES = 5\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "NEO4J_URL=\"neo4j://35.224.138.50:7687\"\n",
    "NEO4J_USERNAME=\"neo4j\"\n",
    "NEO4J_PASSWORD=\"neo4j@123456\"\n",
    "DATABASE_URL = \"postgresql+psycopg2://jktech:123456@35.224.138.50/ekedb\"\n",
    "DATABASE_URL_GLOSSARY_TABLE = \"postgresql://jktech:123456@localhost:5432/ekedb\"\n",
    "RETRIEVAL_MAX_RETRIES = 2\n",
    "CONNECTION_TIMEOUT = 120\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"]=OPENAI_API_KEY\n",
    "PG_HOST=\"35.224.138.50\"\n",
    "PG_PORT=\"5432\"\n",
    "PG_USER=\"jktech\"\n",
    "PG_PASSWORD=\"123456\"\n",
    "PG_DATABASE=\"ekedb\"\n",
    "PG_SCHEMA=\"public\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glossary Table and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dummy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glossary_data = [\n",
    "    {\n",
    "        \"glossary_name\": \"Agreement\",\n",
    "        \"glossary_definition\": \"Contract, Accord, Arrangement, Deal, Pact, Understanding, Treaty\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Party\",\n",
    "        \"glossary_definition\": \"Participant, Entity, Side, Stakeholder, Signatory, Actor, Faction\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Services\",\n",
    "        \"glossary_definition\": \"Assistance, Work, Aid, Support, Help, Tasks, Operations\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Deliverable\",\n",
    "        \"glossary_definition\": \"Output, Result, Product, Submission, Item, Goods, Contribution\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Charges\",\n",
    "        \"glossary_definition\": \"Fees, Costs, Expenses, Payments, Rates, Tariffs, Pricing\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Effective Date\",\n",
    "        \"glossary_definition\": \"Start Date, Commencement Date, Inception Date, Onset Date, Launch Date, Activation Date, Begin Date\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Term\",\n",
    "        \"glossary_definition\": \"Duration, Period, Span, Timeframe, Interval, Tenure, Validity\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Termination\",\n",
    "        \"glossary_definition\": \"End, Conclusion, Cessation, Closure, Expiration, Cancellation, Discontinuation\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Confidentiality\",\n",
    "        \"glossary_definition\": \"Privacy, Secrecy, Discretion, Non-disclosure, Protection, Security, Confidentialness\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Intellectual Property\",\n",
    "        \"glossary_definition\": \"IP, Creative Rights, Proprietary Rights, Innovation Rights, Ownership, Patents, Trademarks\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Affiliate\",\n",
    "        \"glossary_definition\": \"Subsidiary, Branch, Partner, Division, Sister Company, Associated Entity, Joint Venture\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Statement of Work (SOW)\",\n",
    "        \"glossary_definition\": \"Scope of Work, Project Agreement, Task Order, Work Specification, Job Specification, Assignment, Work Statement\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Compliance\",\n",
    "        \"glossary_definition\": \"Adherence, Conformity, Obedience, Accordance, Observance, Abidance, Alignment\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Dispute\",\n",
    "        \"glossary_definition\": \"Conflict, Controversy, Argument, Disagreement, Issue, Debate, Discrepancy\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Warranty\",\n",
    "        \"glossary_definition\": \"Guarantee, Assurance, Promise, Covenant, Commitment, Pledge, Surety\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Invoicing\",\n",
    "        \"glossary_definition\": \"Billing, Charging, Invoice Issuance, Invoice Submission, Account Render, Payment Request, Debit Note\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Liability\",\n",
    "        \"glossary_definition\": \"Responsibility, Obligation, Accountability, Burden, Duty, Onus, Debt\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Force Majeure\",\n",
    "        \"glossary_definition\": \"Act of God, Unforeseen Event, Uncontrollable Event, Natural Disaster, Emergency, Calamity, Catastrophe\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Jurisdiction\",\n",
    "        \"glossary_definition\": \"Authority, Power, Control, Legal Power, Sovereignty, Rule, Dominion\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Amendment\",\n",
    "        \"glossary_definition\": \"Revision, Change, Modification, Alteration, Update, Adjustment, Correction\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Subcontractor\",\n",
    "        \"glossary_definition\": \"Sub supplier, Service Provider, Vendor, Subcontracted Entity, Third-party, Subconsultant, External Contractor\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Material\",\n",
    "        \"glossary_definition\": \"Document, Resource, Data, Information, Content, Record, File\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Breach\",\n",
    "        \"glossary_definition\": \"Violation, Infringement, Noncompliance, Contravention, Infraction, Breaking, Transgression\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Notice\",\n",
    "        \"glossary_definition\": \"Notification, Alert, Intimation, Announcement, Warning, Communication, Advisory\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Obligation\",\n",
    "        \"glossary_definition\": \"Duty, Responsibility, Requirement, Commitment, Task, Charge, Assignment\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Resolution\",\n",
    "        \"glossary_definition\": \"Settlement, Solution, Decision, Conclusion, Outcome, Determination, Verdict\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Performance\",\n",
    "        \"glossary_definition\": \"Execution, Fulfillment, Completion, Delivery, Conduct, Achievement, Carrying Out\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Assignment\",\n",
    "        \"glossary_definition\": \"Transfer, Delegation, Allotment, Allocation, Reassignment, Reallocation, Substitution\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Representation\",\n",
    "        \"glossary_definition\": \"Assertion, Declaration, Statement, Warranty, Guarantee, Pledge, Allegation\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Arbitration\",\n",
    "        \"glossary_definition\": \"Mediation, Adjudication, Settlement, Resolution, Neutral Judgment, Dispute Resolution, Tribunal\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Default\",\n",
    "        \"glossary_definition\": \"Non-performance, Failure, Breach, Neglect, Noncompliance, Lapse, Nonfulfillment\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Contract Period\",\n",
    "        \"glossary_definition\": \"Duration, Term, Length, Span, Interval, Cycle, Timeline\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Dispute Resolution\",\n",
    "        \"glossary_definition\": \"Conflict Resolution, Problem Solving, Dispute Settlement, Mediation, Arbitration, Settlement, Negotiation\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Exclusivity\",\n",
    "        \"glossary_definition\": \"Sole Rights, Unique Rights, Non-Compete, Special Rights, Sole Agreement, Exclusive Deal, Single-source\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Governing Jurisdiction\",\n",
    "        \"glossary_definition\": \"Legal Venue, Forum, Court, Legal Authority, Legal Domain, Venue, Legal System\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Notice Period\",\n",
    "        \"glossary_definition\": \"Notification Period, Advance Notice, Lead Time, Forewarning, Pre-notification, Prior Notice, Advance Period\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Renewal\",\n",
    "        \"glossary_definition\": \"Extension, Continuation, Reinstatement, Prolongation, Reauthorization, Continuity, Reaffirmation\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Settlement Amount\",\n",
    "        \"glossary_definition\": \"Compensation, Payout, Financial Settlement, Award, Disbursement, Payment, Reparation\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Third-Party\",\n",
    "        \"glossary_definition\": \"External Party, Non-party, Outsider, Independent Party, Subcontractor, Vendor, Service Provider\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Confidential Disclosure\",\n",
    "        \"glossary_definition\": \"Private Information Sharing, Non-public Information, Sensitive Disclosure, Restricted Information, Secret Information, Confidential Sharing, Protected Disclosure\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Fees\",\n",
    "        \"glossary_definition\": \"Charges, Costs, Rates, Tariffs, Payments, Expenses, Compensation\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Guarantee\",\n",
    "        \"glossary_definition\": \"Assurance, Warranty, Pledge, Promise, Security, Surety, Commitment\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Indemnification\",\n",
    "        \"glossary_definition\": \"Compensation, Reimbursement, Protection, Reparation, Coverage, Safeguard, Restitution\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Limitation of Liability\",\n",
    "        \"glossary_definition\": \"Liability Cap, Liability Restriction, Liability Ceiling, Damages Limit, Liability Limit, Financial Cap, Liability Boundaries\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Payment Terms\",\n",
    "        \"glossary_definition\": \"Payment Schedule, Payment Conditions, Payment Arrangement, Payment Method, Billing Terms, Payment Policy, Financial Terms\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Performance Bond\",\n",
    "        \"glossary_definition\": \"Surety Bond, Guarantee Bond, Compliance Bond, Performance Guarantee, Security Bond, Assurance Bond, Bond of Performance\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Prior Consent\",\n",
    "        \"glossary_definition\": \"Pre-approval, Authorization, Permission, Agreement, Acknowledgment, Endorsement, Sanction\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Product Liability\",\n",
    "        \"glossary_definition\": \"Defective Product Liability, Consumer Protection, Product Safety, Product Responsibility, Liability for Defects, Warranty Liability, Manufacturer's Liability\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Purchaser\",\n",
    "        \"glossary_definition\": \"Buyer, Client, Customer, Acquirer, Recipient, Consumer\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Representations and Warranties\",\n",
    "        \"glossary_definition\": \"Statements and Assurances, Guarantees, Declarations, Promises, Affirmations, Assertions, Confirmations\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Service Agreement\",\n",
    "        \"glossary_definition\": \"Service Contract, Service Terms, Service Arrangement, Service Provision, Service Terms of Engagement, Service Commitment, Service Understanding\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Settlement Agreement\",\n",
    "        \"glossary_definition\": \"Resolution Agreement, Compromise Agreement, Agreement to Settle, Dispute Settlement Agreement, Compromise, Settlement Arrangement, Final Agreement\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Terms and Conditions\",\n",
    "        \"glossary_definition\": \"Contractual Terms, Provisions, Clauses, Conditions, Rules, Requirements, Terms\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Transfer of Rights\",\n",
    "        \"glossary_definition\": \"Assignment of Rights, Conveyance, Delegation, Transfer of Ownership, Cession, Transfer of Interests, Rights Assignment\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Warranty Period\",\n",
    "        \"glossary_definition\": \"Guarantee Period, Coverage Term, Warranty Term, Protection Period, Validity Period, Assurance Term, Service Term\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Work Product\",\n",
    "        \"glossary_definition\": \"Deliverables, Output, Results, Production, Created Material, Generated Work, Developed Product\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Written Notice\",\n",
    "        \"glossary_definition\": \"Formal Notification, Written Communication, Written Advisory, Official Notice, Documented Notice, Written Alert, Written Intimation\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Ullage\",\n",
    "        \"glossary_definition\": \"The empty space in a container, often used in shipping or storage to indicate unfilled capacity.\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Shrinkage\",\n",
    "        \"glossary_definition\": \"A reduction in size, quantity, or value, often due to loss, theft, or natural factors.\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Turnover Ratio\",\n",
    "        \"glossary_definition\": \"A financial metric that measures how efficiently a company utilizes its assets to generate sales.\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Non-Compete Clause\",\n",
    "        \"glossary_definition\": \"A contractual provision restricting one party from engaging in business activities that compete with another party.\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Force Majeure Clause\",\n",
    "        \"glossary_definition\": \"A contract clause that frees parties from obligations due to extraordinary events beyond their control.\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Indemnity Clause\",\n",
    "        \"glossary_definition\": \"A clause providing compensation for losses or damages incurred by one party due to the actions of another.\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Hold Harmless Agreement\",\n",
    "        \"glossary_definition\": \"An agreement where one party agrees not to hold the other party liable for any damages or claims.\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Severability Clause\",\n",
    "        \"glossary_definition\": \"A provision stating that if part of the contract is invalid, the remaining sections still apply.\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Arbitration Clause\",\n",
    "        \"glossary_definition\": \"A clause that requires disputes to be resolved through arbitration rather than litigation.\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Acceleration Clause\",\n",
    "        \"glossary_definition\": \"A contract provision that allows a lender to demand early repayment of a loan under specific conditions.\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Liquidated Damages\",\n",
    "        \"glossary_definition\": \"A pre-agreed amount of compensation for breach of contract, intended to cover losses without litigation.\"\n",
    "    },\n",
    "    {\n",
    "        \"glossary_name\": \"Escrow Agreement\",\n",
    "        \"glossary_definition\": \"A financial arrangement where a third party holds funds or assets until certain conditions are met.\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert Data to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import json\n",
    "from datetime import datetime\n",
    "from sqlalchemy import Column, String, Text, DateTime, Boolean, Integer, create_engine, JSON\n",
    "from sqlalchemy.dialects.postgresql import UUID, ARRAY\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "# Step 1: Initialize the database\n",
    "def initialize_database():\n",
    "    engine = create_engine(DATABASE_URL_GLOSSARY_TABLE)\n",
    "    Base.metadata.create_all(bind=engine)\n",
    "\n",
    "# Step 2: Updated Glossary Model using JSONB for embeddings\n",
    "class Glossary(Base):\n",
    "    __tablename__ = 'glossary'\n",
    "\n",
    "    term_id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4, nullable=False)\n",
    "    user_id = Column(UUID(as_uuid=True), nullable=False)\n",
    "    term_name = Column(String(255), nullable=False)\n",
    "    term_description = Column(Text, nullable=True)\n",
    "    metric_sql_query = Column(Text, nullable=True)\n",
    "\n",
    "    # Contextual Fields\n",
    "    db_names = Column(ARRAY(Text), nullable=True)\n",
    "    schema_names = Column(ARRAY(Text), nullable=True)\n",
    "    document_names = Column(ARRAY(Text), nullable=True)\n",
    "\n",
    "    # Embedding Fields using JSONB\n",
    "    embedding_of_name = Column(JSON, nullable=True)\n",
    "    embedding_of_description = Column(JSON, nullable=True)\n",
    "    embedding_of_sql_query = Column(JSON, nullable=True)\n",
    "\n",
    "    version = Column(Integer, default=1, nullable=False)\n",
    "    is_active = Column(Boolean, default=True, nullable=False)\n",
    "    created_at = Column(DateTime, default=datetime.utcnow, nullable=False)\n",
    "    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow, nullable=False)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"<Glossary(term_id={self.term_id}, term_name={self.term_name}, user_id={self.user_id})>\"\n",
    "\n",
    "# Step 3: Initialize the database (create table if not present)\n",
    "initialize_database()\n",
    "\n",
    "# Step 4: Create a session to interact with the database\n",
    "def get_session_kpi():\n",
    "    engine = create_engine(DATABASE_URL_GLOSSARY_TABLE)\n",
    "    Session = sessionmaker(bind=engine)\n",
    "    return Session()\n",
    "\n",
    "# Step 5: Embedding generation function\n",
    "def generate_embedding(text, client, model):\n",
    "    response = client.embeddings.create(input=text, model=model)\n",
    "    embedding = response.data[0].embedding\n",
    "    return embedding  # Return the embedding as a list of floats\n",
    "\n",
    "# Step 6: Function to insert glossary data\n",
    "def insert_glossary_data(session, glossary_data, client, embedding_model, user_id):\n",
    "    \"\"\"\n",
    "    Insert glossary data into the database.\n",
    "\n",
    "    Args:\n",
    "        session: SQLAlchemy session object.\n",
    "        glossary_data: List of glossary items to insert. Each item should be a dictionary with keys:\n",
    "                       'glossary_name', 'glossary_definition', 'db_names', 'schema_names', 'document_names'.\n",
    "        client: Embedding client (e.g., OpenAI client).\n",
    "        embedding_model: Model to use for generating embeddings.\n",
    "        user_id: UUID of the user adding the glossary data.\n",
    "    \"\"\"\n",
    "    for item in glossary_data:\n",
    "        term_name = item.get('glossary_name', '').strip()\n",
    "        term_description = item.get('glossary_definition', '').strip()\n",
    "        db_names = item.get('db_names', [])\n",
    "        schema_names = item.get('schema_names', [])\n",
    "        document_names = item.get('document_names', [])\n",
    "\n",
    "        # Generate embeddings\n",
    "        embedding_name = generate_embedding(term_name, client, embedding_model) if term_name else None\n",
    "        embedding_description = generate_embedding(term_description, client, embedding_model) if term_description else None\n",
    "\n",
    "        # Create glossary entry\n",
    "        glossary_entry = Glossary(\n",
    "            term_id=uuid.uuid4(),\n",
    "            user_id=user_id,\n",
    "            term_name=term_name,\n",
    "            term_description=term_description,\n",
    "            metric_sql_query=None,\n",
    "            db_names=db_names if isinstance(db_names, list) else [],\n",
    "            schema_names=schema_names if isinstance(schema_names, list) else [],\n",
    "            document_names=document_names if isinstance(document_names, list) else [],\n",
    "            embedding_of_name=embedding_name,\n",
    "            embedding_of_description=embedding_description,\n",
    "            embedding_of_sql_query=None,\n",
    "            version=1,\n",
    "            is_active=True,\n",
    "            created_at=datetime.utcnow(),\n",
    "            updated_at=datetime.utcnow()\n",
    "        )\n",
    "\n",
    "        # Add and commit entry\n",
    "        session.add(glossary_entry)\n",
    "\n",
    "    session.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Only Once to insert Dummy data then comment out this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert_glossary_data(session=get_session_kpi(),client=client,embedding_model=EMBEDDING_MODEL, glossary_data=glossary_data, user_id=\"50deb7bb-9f71-4ec8-8fb7-7043ed5d7ab5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.ERROR)\n",
    "\n",
    "\n",
    "def init_db(DATABASE_URL=DATABASE_URL):\n",
    "    # Database configuration\n",
    "    # DATABASE_URL = \"postgresql+asyncpg://jktech:123456@localhost/ekedb\"\n",
    "    # DATABASE_URL = \"postgresql+psycopg2://jktech:123456@35.224.138.50/ekedb\"\n",
    "\n",
    "    # Set up SQLAlchemy\n",
    "    Base = declarative_base()\n",
    "    engine = create_engine(DATABASE_URL, echo=True)\n",
    "    SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n",
    "    return Base, SessionLocal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.types import UserDefinedType\n",
    "from sqlalchemy import Column, Text, ARRAY\n",
    "from sqlalchemy.dialects.postgresql import UUID, JSONB\n",
    "from typing import List, Dict\n",
    "import json\n",
    "from datetime import date, datetime\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from decimal import Decimal\n",
    "import base64\n",
    "import hashlib\n",
    "import os\n",
    "import secrets\n",
    "import string\n",
    "import uuid\n",
    "\n",
    "import jwt\n",
    "from Crypto.Cipher import AES\n",
    "from Crypto.Random import get_random_bytes\n",
    "from Crypto.Util.Padding import pad, unpad\n",
    "from dotenv import load_dotenv\n",
    "from fastapi import HTTPException, Request\n",
    "from fastapi.security import HTTPAuthorizationCredentials, HTTPBearer\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.ERROR)\n",
    "\n",
    "Base, _ = init_db(DATABASE_URL=DATABASE_URL)\n",
    "\n",
    "\n",
    "class Vector(UserDefinedType):\n",
    "    def get_col_spec(self):\n",
    "        return \"vector(1536)\"  # Adjust the dimension based on the embedding size\n",
    "\n",
    "\n",
    "class Embedding(Base):\n",
    "    __tablename__ = \"combined_data\"\n",
    "    id = Column(UUID(as_uuid=True), primary_key=True, index=True)\n",
    "    source_type = Column(Text, nullable=False)\n",
    "    chunk_content = Column(Text, nullable=False)\n",
    "    embedding = Column(Vector, nullable=False)\n",
    "    keywords = Column(ARRAY(Text), nullable=True)\n",
    "    keywords_combined = Column(Text, nullable=True)\n",
    "    roles = Column(ARRAY(Text), nullable=True)\n",
    "    summary = Column(Text, nullable=True)\n",
    "    document_name = Column(Text, nullable=True)\n",
    "    document_url = Column(Text, nullable=True)\n",
    "    table_name = Column(Text, nullable=True)\n",
    "    database_name = Column(Text, nullable=True)\n",
    "    schema_name = Column(Text, nullable=True)\n",
    "    table_ddl = Column(Text, nullable=True)\n",
    "    table_ddl_string = Column(Text, nullable=True)\n",
    "    related_tables = Column(ARRAY(Text), nullable=True)\n",
    "    page_number = Column(Text, nullable=True)\n",
    "    entities = Column(ARRAY(Text), nullable=True)\n",
    "    entities_combined = Column(Text, nullable=True)\n",
    "    questions = Column(ARRAY(Text), nullable=True)\n",
    "    questions_combined = Column(Text, nullable=True)\n",
    "    questions_embedding = Column(Vector, nullable=True)\n",
    "    node_metadata = Column(JSONB, nullable=True)\n",
    "    previous_chunk_id = Column(UUID(as_uuid=True), nullable=True)\n",
    "    next_chunk_id = Column(UUID(as_uuid=True), nullable=True)\n",
    "\n",
    "\n",
    "class CustomJSONEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, (date, datetime)):\n",
    "            return obj.isoformat()\n",
    "        elif isinstance(obj, Decimal):\n",
    "            return float(obj)\n",
    "        return super().default(obj)\n",
    "\n",
    "\n",
    "class Baseline:\n",
    "    baseline_dict: List[Dict[str, str]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pydantic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "\n",
    "class UserQuery(BaseModel):\n",
    "    \"\"\"\n",
    "    UserQuery represents a model containing a user's query.\n",
    "\n",
    "    Attributes:\n",
    "        query (str): The query string provided by the user.\n",
    "    \"\"\"\n",
    "\n",
    "    query: str\n",
    "\n",
    "\n",
    "class Responses(BaseModel):\n",
    "    keyword_question_emb: Dict\n",
    "    keyword_chunk_emb: Dict\n",
    "    entity_chunk_emb: Dict\n",
    "    best_answer: Dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "class Neo4jDatabase:\n",
    "    def __init__(self, uri, user, password):\n",
    "        self.uri = uri\n",
    "        self.user = user\n",
    "        self.password = password\n",
    "        self.driver = None\n",
    "\n",
    "    def connect(self):\n",
    "        if self.driver is None:\n",
    "            self.driver = GraphDatabase.driver(self.uri, auth=(self.user, self.password))\n",
    "\n",
    "    def close(self):\n",
    "        if self.driver is not None:\n",
    "            self.driver.close()\n",
    "            self.driver = None\n",
    "\n",
    "    def run_query(self, query, parameters=None):\n",
    "        if parameters is None:\n",
    "            parameters = {}\n",
    "        with self.driver.session() as session:\n",
    "            result = session.run(query, parameters)\n",
    "            return [record for record in result]\n",
    "        \n",
    "    def get_keyword_node_ids(self, keyword, chunk_node_id, retriever_flow):\n",
    "        if retriever_flow == \"keyword\":\n",
    "            query = \"\"\"\n",
    "            MATCH (k:Keyword)-[]-(n)\n",
    "            WHERE (toLower(k.name) CONTAINS toLower($keyword) OR toLower($keyword) CONTAINS toLower(k.name)) \n",
    "              AND (n:UnstructuredChunkNode OR n:StructuredChunkNode) \n",
    "              AND n.id = $chunk_node_id\n",
    "            RETURN k.id AS node_id\n",
    "            \"\"\"\n",
    "        elif retriever_flow == \"entity\":\n",
    "            query = \"\"\"\n",
    "            MATCH (e:Entity)-[]-(n)\n",
    "            WHERE (toLower(e.name) CONTAINS toLower($keyword) OR toLower($keyword) CONTAINS toLower(e.name)) \n",
    "              AND (n:UnstructuredChunkNode OR n:StructuredChunkNode) \n",
    "              AND n.id = $chunk_node_id\n",
    "            RETURN e.id AS node_id\n",
    "            \"\"\"\n",
    "        elif retriever_flow == \"question\":\n",
    "            query = \"\"\"\n",
    "            MATCH (e:SampleQuestion)-[]-(n)\n",
    "            WHERE (n:UnstructuredChunkNode OR n:StructuredChunkNode) \n",
    "              AND n.id = $chunk_node_id\n",
    "            RETURN e.id AS node_id\n",
    "            \"\"\"\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "        with self.driver.session() as session:\n",
    "            result = session.run(query, keyword=keyword, chunk_node_id=chunk_node_id)\n",
    "            node_ids = [record['node_id'] for record in result]\n",
    "            return node_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import text\n",
    "import multiprocessing\n",
    "import traceback\n",
    "import pymysql\n",
    "import json\n",
    "import openai\n",
    "import os\n",
    "import time\n",
    "\n",
    "Base, SessionLocal = init_db()\n",
    "\n",
    "empty_cancel_response = {\n",
    "    \"message\": \"\",\n",
    "    \"data\": [],\n",
    "    \"execution_time\": None,\n",
    "}\n",
    "\n",
    "\n",
    "def find_similar_embeddings_by_keyword_and_chunk_embedding(\n",
    "    embedding, joined_words, roles, run_id=None, source_type=None\n",
    "):\n",
    "    try:\n",
    "        with SessionLocal() as session:\n",
    "            query = text(f\"\"\"\n",
    "                SELECT id::text, source_id::text, chunk_content, summary, keywords, entities, questions, keywords_combined, document_name, document_url, table_name, database_name, page_number,\n",
    "                       1 - (embedding <=> :embedding) AS similarity\n",
    "                FROM combined_data\n",
    "                WHERE to_tsvector('english', keywords_combined) @@ to_tsquery('english', :joined_keywords) \n",
    "                AND roles && :roles\n",
    "                {'AND source_type = :source_type' if source_type else ''}\n",
    "                ORDER BY similarity DESC\n",
    "                LIMIT {NO_OF_CLOSEST_NODES};\n",
    "            \"\"\")\n",
    "\n",
    "            result = session.execute(\n",
    "                query,\n",
    "                {\n",
    "                    \"embedding\": embedding,\n",
    "                    \"joined_keywords\": joined_words,\n",
    "                    \"roles\": roles,\n",
    "                    \"source_type\": source_type if source_type else None\n",
    "                },\n",
    "            )\n",
    "            rows = result.fetchall()\n",
    "            return rows\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching - {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "def find_connection_details(connection_id: str):\n",
    "    try:\n",
    "        with SessionLocal() as session:\n",
    "            query = text(f\"\"\"\n",
    "                SELECT * from connection_source\n",
    "                WHERE connection_id='{connection_id}'\n",
    "                LIMIT 5;\n",
    "            \"\"\")\n",
    "            result = session.execute(query)\n",
    "            rows = result.fetchall()\n",
    "            print(\"Find connection Detail Rows: \", rows)\n",
    "            return rows[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching - {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "def get_connection_details():\n",
    "    try:\n",
    "        with SessionLocal() as session:\n",
    "            query = text(f\"\"\"\n",
    "                SELECT * from connection_source;\n",
    "            \"\"\")\n",
    "            result = session.execute(query)\n",
    "            rows = result.fetchall()\n",
    "            return rows\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching - {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "def find_similar_embeddings_by_entities_and_chunk_embedding(\n",
    "    embedding, joined_words, roles, run_id=None, source_type=None\n",
    "):\n",
    "    try:\n",
    "        with SessionLocal() as session:\n",
    "            query = text(f\"\"\"\n",
    "                SELECT id::text, source_id::text, chunk_content, summary, keywords, entities, questions, keywords_combined, document_name, document_url, table_name, database_name, page_number,\n",
    "                       1 - (embedding <=> :embedding) AS similarity\n",
    "                FROM combined_data\n",
    "                WHERE to_tsvector('english', entities_combined) @@ to_tsquery('english', :joined_entities)\n",
    "                AND roles && :roles\n",
    "                {'AND source_type = :source_type' if source_type else ''}\n",
    "                ORDER BY similarity DESC\n",
    "                LIMIT {NO_OF_CLOSEST_NODES};\n",
    "            \"\"\")\n",
    "            result = session.execute(\n",
    "                query,\n",
    "                {\n",
    "                    \"embedding\": embedding,\n",
    "                    \"joined_entities\": joined_words,\n",
    "                    \"roles\": roles,\n",
    "                    \"source_type\": source_type if source_type else None\n",
    "                },\n",
    "            )\n",
    "            rows = result.fetchall()\n",
    "            return rows\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching - {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "def find_similar_embeddings_by_keyword_and_questions_embedding(\n",
    "    embedding, joined_words, roles, run_id=None, source_type=None\n",
    "):\n",
    "    try:\n",
    "        with SessionLocal() as session:\n",
    "            query = text(f\"\"\"\n",
    "                SELECT id::text, source_id::text, chunk_content, summary, keywords, entities, questions, keywords_combined, document_name, document_url, table_name, database_name, page_number,\n",
    "                       1 - (questions_embedding <=> :questions_embedding) AS similarity\n",
    "                FROM combined_data\n",
    "                WHERE roles && :roles\n",
    "                {'AND source_type = :source_type' if source_type else ''}\n",
    "                ORDER BY similarity DESC\n",
    "                LIMIT {NO_OF_CLOSEST_NODES};\n",
    "            \"\"\")\n",
    "            result = session.execute(\n",
    "                query,\n",
    "                {\n",
    "                    \"questions_embedding\": embedding,\n",
    "                    \"joined_keywords\": joined_words,\n",
    "                    \"roles\": roles,\n",
    "                    \"source_type\": source_type if source_type else None\n",
    "                },\n",
    "            )\n",
    "            rows = result.fetchall()\n",
    "            return rows\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching - {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "def refiner(query, sql, error, exception_class):\n",
    "    refiner_template = \"\"\"\n",
    "【Instruction】\n",
    "When executing SQL below, some errors occurred, please fix up SQL based on query and database info.\n",
    "Solve the task step by step if you need to. Using SQL format in the code block, and indicate script type in the code block.\n",
    "When you find an answer, verify the answer carefully. Include verifiable evidence in your response if possible. Also always keep in mind that any LIKE search needs to be case insensitive for most accurate result.\n",
    "【Constraints】\n",
    "- In `SELECT <column>`, just select needed columns in the 【Question】 without any unnecessary column or value\n",
    "- In `FROM <table>` or `JOIN <table>`, do not include unnecessary table\n",
    "- If use max or min func, `JOIN <table>` FIRST, THEN use `SELECT MAX(<column>)` or `SELECT MIN(<column>)`\n",
    "- If [Value examples] of <column> has 'None' or None, use `JOIN <table>` or `WHERE <column> is NOT NULL` is better\n",
    "- If use `ORDER BY <column> ASC|DESC`, add `GROUP BY <column>` before to select distinct values\n",
    "- If the exception is 'This result object does not return rows. It has been closed automatically.' i.e the provided sql has comments, remove them.\n",
    "【Query】\n",
    "-- {query}\n",
    "【old SQL】\n",
    "```sql\n",
    "{sql}\n",
    "```\n",
    "【SQL】 \n",
    "{sql_error}\n",
    "【Exception class】\n",
    "{exception_class}\n",
    "\n",
    "Now please fixup old SQL and generate new SQL again.give the sql in json format\n",
    "using below template\n",
    "```json\n",
    "{{\n",
    "\"query\":\"\"\n",
    "}}\n",
    "```\n",
    "\"\"\"\n",
    "    refiner_prompt = refiner_template.format(\n",
    "        query=query, sql=sql, sql_error=error, exception_class=exception_class\n",
    "    )\n",
    "\n",
    "    response = (\n",
    "        openai.chat.completions.create(\n",
    "            model=OPENAI_MODEL_4O,\n",
    "            messages=[{\"role\": \"user\", \"content\": refiner_prompt}],\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "        )\n",
    "        .choices[0]\n",
    "        .message.content\n",
    "    )\n",
    "\n",
    "    # print(response)\n",
    "    response = json.loads(response)\n",
    "    return response[\"query\"]\n",
    "\n",
    "\n",
    "def kill_overtime_query(connection, timeout):\n",
    "    # This function would use the specific database command to kill a query that exceeds the timeout\n",
    "    # This implementation may vary based on the database being used.\n",
    "    with connection.cursor() as cursor:\n",
    "        cursor.execute(\"SHOW FULL PROCESSLIST\")\n",
    "        processlist = cursor.fetchall()\n",
    "        for process in processlist:\n",
    "            if (\n",
    "                process[\"Time\"] >= timeout\n",
    "                and process[\"Info\"]\n",
    "                and process[\"Info\"].startswith(\"SELECT\")\n",
    "            ):\n",
    "                print(f\"Killing process {process['Id']}\")\n",
    "                cursor.execute(f\"KILL QUERY {process['Id']}\")\n",
    "\n",
    "\n",
    "def execute_query_in_process(query, connection_details, result_queue, run_id=None):\n",
    "    connection = pymysql.connect(\n",
    "        host=connection_details.host,\n",
    "        user=connection_details.user,\n",
    "        password=connection_details.password,\n",
    "        database=connection_details.database,\n",
    "        cursorclass=pymysql.cursors.DictCursor,\n",
    "    )\n",
    "    kill_overtime_query(\n",
    "        connection=connection, timeout=CONNECTION_TIMEOUT\n",
    "    )\n",
    "\n",
    "    # Read the contents of the file\n",
    "    # file_path = 'app/constants.py'\n",
    "    # with open(file_path, 'r') as file:\n",
    "    #     lines = file.readlines()\n",
    "\n",
    "    # # Modify the line that starts with 'CONNECTION_DETAIL'\n",
    "    # with open(file_path, 'w') as file:\n",
    "    #     for line in lines:\n",
    "    #         if line.strip().startswith('CONNECTION_DETAIL'):\n",
    "    #             file.write(f\"\"\"CONNECTION_DETAIL=\"{','.join([connection_details.host, connection_details.user, connection_details.password, connection_details.database])}\"\\n\"\"\")\n",
    "    #         else:\n",
    "    #             file.write(line)\n",
    "\n",
    "    try:\n",
    "        with connection.cursor() as cursor:\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "            cursor.execute(query)\n",
    "            rows = cursor.fetchall()\n",
    "            execution_time = time.time() - start_time\n",
    "            result_queue.put({\"rows\": rows, \"execution_time\": execution_time})\n",
    "    except Exception as e:\n",
    "        result_queue.put(e)\n",
    "    finally:\n",
    "        connection.close()\n",
    "\n",
    "\n",
    "def execute_query(query, connection_details, user_sessions=None, run_id=None):\n",
    "    result_queue = multiprocessing.Queue()\n",
    "    timeout = CONNECTION_TIMEOUT  # Default to 120 seconds if not set\n",
    "    process = multiprocessing.Process(\n",
    "        target=execute_query_in_process,\n",
    "        args=(query, connection_details, result_queue, run_id),\n",
    "    )\n",
    "    process.start()\n",
    "    process.join(timeout)\n",
    "\n",
    "    if process.is_alive():\n",
    "        process.terminate()\n",
    "        process.join()\n",
    "        message = \"Query execution time exceeded the timeout limit.\"\n",
    "        print(message)\n",
    "        return json.dumps(\n",
    "            {\"message\": message, \"data\": [], \"execution_time\": None},\n",
    "            cls=CustomJSONEncoder,\n",
    "        )\n",
    "\n",
    "    if not result_queue.empty():\n",
    "        result = result_queue.get()\n",
    "        if isinstance(result, Exception):\n",
    "            message = \"An error occurred during query execution.\"\n",
    "            print(message)\n",
    "\n",
    "            error_result = {\n",
    "                \"message\": message,\n",
    "                \"error\": str(result),\n",
    "                \"data\": [],\n",
    "                \"execution_time\": None,\n",
    "            }\n",
    "            return json.dumps(error_result, cls=CustomJSONEncoder)\n",
    "        else:\n",
    "            rows = result[\"rows\"]\n",
    "            execution_time = result[\"execution_time\"]\n",
    "            if len(rows) == 0:\n",
    "                result_dict = {\n",
    "                    \"message\": \"No results found.\",\n",
    "                    \"data\": [],\n",
    "                    \"execution_time\": execution_time,\n",
    "                }\n",
    "            elif len(rows) > 20:\n",
    "                result_dict = {\n",
    "                    \"message\": \"The results are partial. Only the first 20 rows are displayed.\",\n",
    "                    \"data\": rows[:20],\n",
    "                    \"execution_time\": execution_time,\n",
    "                }\n",
    "            elif len(rows) > 1:\n",
    "                result_dict = {\n",
    "                    \"message\": \"All results are displayed.\",\n",
    "                    \"data\": rows,\n",
    "                    \"execution_time\": execution_time,\n",
    "                }\n",
    "            else:\n",
    "                row = rows[0]\n",
    "                result_dict = {\n",
    "                    \"message\": \"Single result found.\",\n",
    "                    \"data\": [row],\n",
    "                    \"execution_time\": execution_time,\n",
    "                }\n",
    "            return json.dumps(result_dict, cls=CustomJSONEncoder)\n",
    "\n",
    "    return json.dumps(\n",
    "        {\n",
    "            \"message\": \"No result returned from the process.\",\n",
    "            \"data\": [],\n",
    "            \"execution_time\": None,\n",
    "        },\n",
    "        cls=CustomJSONEncoder,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "KEYWORDS_ACTIONS_INFO_TYPE_EXTRACTOR = \"\"\"\n",
    "        You are a retail domain expert, and your task is to analyze the question.\n",
    "\n",
    "        Break the given question into multiple (two or more) linked actions such that the answer from the first action can be used in the next action to provide a more comprehensive answer.\n",
    "        For each action, provide unique keywords and describe the type of information requested. \n",
    "        This way, the output is more structured and actionable.\n",
    "\n",
    "        The retail system we have, has stored following data in a structured RDBMS system:\n",
    "        - Items (also known as products, sku) and its categorization\n",
    "        - Orders and their associated details along like cuustomet details, order date, ordered items, promotions applied, etc.\n",
    "        - Stores and their inventory details including indicators for item's stock availability\n",
    "        - Warehouses and their inventory details including indicators for item's stock availability\n",
    "        - Promotions and their details like items for which promotions is applied, promotion precentage start and end date, etc.\n",
    "        - Customers and their details like name, address, contact details, etc.\n",
    "        - Customers, Warehouses, Stores also have city and country information associated with it.\n",
    "\n",
    "        If the action relates to fetching information from any of the above, please identify the action as STRUCTURED.\n",
    "        Otherwise the action is UNSTRUCTURED.\n",
    "\n",
    "        Take a look at the following three examples to understand the process better:\n",
    "\n",
    "        Example 1:\n",
    "\n",
    "        What are the total sales amounts and discounts applied for each transaction type in each store?\n",
    "\n",
    "        The linked actions can be:\n",
    "\n",
    "        - Get the list of all stores\n",
    "        - For each store from the above action, get the store details\n",
    "        - Using details of each store from the above action, find a unique list of transaction types\n",
    "        - For every combination of store and transaction type from the above action, find the total sales amount and discounts\n",
    "        - Output a table containing store information, transaction type, total sales amount, and discounts\n",
    "\n",
    "        The output should be:\n",
    "\n",
    "        ```json\n",
    "        [\n",
    "            {{\n",
    "                \"keywords\": [\"store\"],\n",
    "                \"action\": \"Get the list of all stores\",\n",
    "                \"information\": \"Details of all stores\",\n",
    "                \"type\": \"STRUCTURED\"\n",
    "            }},\n",
    "            {{\n",
    "                \"keywords\": [\"store details\"],\n",
    "                \"action\": \"For each store from the above action, get the store details\",\n",
    "                \"information\": \"Details of each store\"\n",
    "                \"type\": \"STRUCTURED\"\n",
    "            }},\n",
    "            {{\n",
    "                \"keywords\": [\"transaction type\"],\n",
    "                \"action\": \"Using details of each store from the above action, find a unique list of transaction types\",\n",
    "                \"information\": \"Details of unique transaction types associated with each store\",\n",
    "                \"type\": \"STRUCTURED\"\n",
    "            }},\n",
    "            {{\n",
    "                \"keywords\": [\"sales amount\", \"discount\", \"transaction type\", \"store\"],\n",
    "                \"action\": \"For every combination of store and transaction type from the above action, find the total sales amount and discounts\",\n",
    "                \"information\": \"Details of total sales and discounts for each store and transaction type\",\n",
    "                \"type\": \"STRUCTURED\"\n",
    "            }}\n",
    "        ]\n",
    "        ```\n",
    "\n",
    "        Example 2:\n",
    "\n",
    "        Identify storage and handling information of product identifier 1234DF555.\n",
    "\n",
    "        The linked actions can be:\n",
    "\n",
    "        - Find out the details of the product identifier 1234DF555\n",
    "        - Based on the details of the product, find storage and handling information\n",
    "\n",
    "        The output should be:\n",
    "        \n",
    "        ```json\n",
    "        [\n",
    "            {{\n",
    "                \"keywords\": [\"product identifier\", \"product details\"],\n",
    "                \"action\": \"Find out the details of the product identifier 1234DF555\",\n",
    "                \"information\": \"Details of product with identifier 1234DF555\",\n",
    "                \"type\": \"STRUCTURED\"\n",
    "            }},\n",
    "            {{\n",
    "                \"keywords\": [\"storage instructions\", \"handling instructions\"],\n",
    "                \"action\": \"Based on the details of the product, find storage and handling information\",\n",
    "                \"information\": \"Details of storage and handling instructions\",\n",
    "                \"type\": \"UNSTRUCTURED\"\n",
    "            }}\n",
    "        ]\n",
    "        ```\n",
    "\n",
    "        Example 3:\n",
    "\n",
    "        Identify the products with the highest and lowest utilization rates.\n",
    "\n",
    "        The linked actions can be:\n",
    "\n",
    "        - Find out how to calculate the utilization rate of a product\n",
    "        - Get a list of all products\n",
    "        - For each product from the above action, get details of the product\n",
    "        - Based on the details of each product, calculate the utilization rate\n",
    "        - Sort products based on the utilization rate\n",
    "        - Output the products with the highest and lowest utilization rates\n",
    "\n",
    "        The output should be:\n",
    "\n",
    "        ```json\n",
    "        [\n",
    "            {{\n",
    "                \"keywords\": [\"utilization rate\", \"KPI\"],\n",
    "                \"action\": \"Find out how to calculate the utilization rate of a product\",\n",
    "                \"information\": \"Details of utilization rate calculation\",\n",
    "                \"type\": \"UNSTRUCTURED\"\n",
    "            }},\n",
    "            {{\n",
    "                \"keywords\": [\"product\"],\n",
    "                \"action\": \"Get list of all products\",\n",
    "                \"information\": \"Details of all products\",\n",
    "                \"type\": \"STRUCTURED\"\n",
    "            }},\n",
    "            {{\n",
    "                \"keywords\": [\"product\", \"utilization rate\"],\n",
    "                \"action\": \"For each product from the above action, get details of the product\",\n",
    "                \"information\": \"Details of utilization rate for a given product\",\n",
    "                \"type\": \"STRUCTURED\"\n",
    "            }},\n",
    "            {{\n",
    "                \"keywords\": [\"utilization rate\", \"highest utilization\", \"lowest utilization\"],\n",
    "                \"action\": \"Sort products based on the utilization rate\",\n",
    "                \"information\": \"Details of highest and lowest utilization rate products\",\n",
    "                \"type\": \"STRUCTURED\"\n",
    "            }}\n",
    "        ]\n",
    "        ```\n",
    "\n",
    "        Special Note on Example 3:\n",
    "\n",
    "        In this example, we have encountered a KPI (Key Performance Indicator) called \"utilization rate\". Hence the first step is to understand how to calculate the utilization rate of a product. So such actions are applicable only in cases where a KPI is mentioned in the question. In other cases, such first actions may not be required.\n",
    "\n",
    "        Important note on generating keywords:\n",
    "        Extract keywords that are relevant to the retail domain.\n",
    "\n",
    "        Question for your analysis:\n",
    "\n",
    "        {question}\n",
    "\n",
    "        Use the template below for output:\n",
    "\n",
    "        ```json\n",
    "        [\n",
    "            {{\n",
    "                \"keywords\": [],\n",
    "                \"action\": \"action text\",\n",
    "                \"information\": \"information text\",\n",
    "                \"type\": \"STRUCTURED/UNSTRUCTURED\"\n",
    "            }}\n",
    "        ]\n",
    "        ```\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "STR_RESPONSE_SYNTHESIZER = \"\"\"\n",
    "Question: {question}\n",
    "SQL: {sql}\n",
    "SQL Response: {response}\n",
    "\n",
    "Task: Based on the provided Question, SQL and SQL Response, synthesize the information to formulate a clear and concise answer. Do not use any additional information apart from what is provided.\n",
    "\n",
    "Use below template for output:\n",
    "\n",
    "```\n",
    "{{\n",
    "    \"answer\": \"\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "KEYWORDS_ACTIONS_INFO_ENTITIES_EXTRACTOR = \"\"\"\n",
    "    Break the given question into multiple (two or more) linked actions such that the answer from the first action\n",
    "    can be used in the next action to provide a more comprehensive answer. Based on the linked actions, provide unique\n",
    "    keywords from all questions, a unique list of types of information requested by each action, and a unique list of\n",
    "    entities mentioned. Take a look at the following two examples to understand the process better:\n",
    "\n",
    "    Example 1:\n",
    "\n",
    "    What are the total sales amounts and discounts applied for each transaction type in each store?\n",
    "\n",
    "    The linked actions can be:\n",
    "\n",
    "    - Get the list of all stores\n",
    "    - For each store from the above action, get the store details\n",
    "    - Using details of each store from the above action, find a unique list of transaction types\n",
    "    - For every combination of store and transaction type from the above action, find the total sales amount and discounts\n",
    "    - Output a table containing store information, transaction type, total sales amount, and discounts\n",
    "\n",
    "    The non-technical keywords can be:\n",
    "\n",
    "    - store\n",
    "    - store details\n",
    "    - transaction type\n",
    "    - discount\n",
    "    - sales amount\n",
    "\n",
    "    Entities can be:\n",
    "\n",
    "    - Stores\n",
    "    - Transaction Types\n",
    "\n",
    "    Type of information requested can be:\n",
    "\n",
    "    - Details of all stores\n",
    "    - Details of unique transaction types associated with each store\n",
    "    - Details of total sales for a given store and transaction type\n",
    "    - Details of discounts for a given store and transaction type\n",
    "\n",
    "    Example 2:\n",
    "\n",
    "    Identify storage and handling information of product identifier 1234DF555.\n",
    "\n",
    "    The linked actions can be:\n",
    "\n",
    "    - Find out the details of the product identifier 1234DF555\n",
    "    - Based on the details of the product, find storage and handling information\n",
    "\n",
    "    The non-technical keywords can be:\n",
    "\n",
    "    - storage instructions\n",
    "    - handling instructions\n",
    "\n",
    "    Entities can be:\n",
    "\n",
    "    - Product Identifier\n",
    "\n",
    "    Type of information requested can be:\n",
    "\n",
    "    - Details of product with identifier 1234DF555\n",
    "    - Details of storage and handling instructions\n",
    "\n",
    "    Example 3:\n",
    "\n",
    "    Identify the products with the highest and lowest utilization rates.\n",
    "\n",
    "    The linked actions can be:\n",
    "\n",
    "    - Find out how to calculate the utilization rate of a product\n",
    "    - Get a list of all products\n",
    "    - For each product from the above action, get details of the product\n",
    "    - Based on the details of each product, calculate utilization rate\n",
    "    - Sort products based on the utilization rate\n",
    "    - Output the products with the highest and lowest utilization rates\n",
    "\n",
    "    The non-technical keywords can be:\n",
    "\n",
    "    - utilization rate\n",
    "    - product\n",
    "\n",
    "    Entities can be:\n",
    "\n",
    "    - Products\n",
    "\n",
    "    Type of information requested can be:\n",
    "\n",
    "    - Details of utilization rate calculation\n",
    "    - Details of all products\n",
    "    - Details of utilization rate for a given product\n",
    "    - Details of highest and lowest utilization rate products\n",
    "\n",
    "    Special Note on Example 3:\n",
    "    ==========================\n",
    "    In this example, we have encountered a KPI (Key Performance Indicator) called \"utilization rate.\" Hence the first step is to understand how to calculate the utilization rate of a product. Such actions are applicable only in cases where a KPI is mentioned in the question. In other cases, such first actions may not be required.\n",
    "\n",
    "    Question for your analysis:\n",
    "\n",
    "    {question}\n",
    "\n",
    "    Use the below template for json output:\n",
    "    \n",
    "    \n",
    "    ```json\n",
    "    {{\n",
    "        \"keywords\":[],\n",
    "        \"actions\":[],\n",
    "        \"information\":[],\n",
    "        \"entities\":[]\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "STR_ACTION_EXECUTOR = \"\"\"\n",
    "        You are a helpful assitant and your task is to execute a given action.\n",
    "        You have been provided outcomes of executions of previous actions. The previous actions outcome will be empty for the first action.\n",
    "        You have also been provided multiple information chunks to execute the current action.\n",
    "        Each information chunk starts with \"Information <number>:\" where <number> is the running number.\n",
    "        Each chunk also has a similarity score associated with it. It is marekd as \"Similarity: <score>\".\n",
    "        Giver higher preference to the information chunk with highest similarity score to answer the question.\n",
    "        You may use all or just one of the information chunks to perform action keeping in mind the note about similarity score above.\n",
    "        To perform the action, you may have to use the information chunks and form a valid PLSQL query that can be executed against a MySQL database.\n",
    "        Do not use any information other than what is provided in the information chunk.\n",
    "        Use only the tables and columns as specified in the DDL that is part of the information chunk.\n",
    "        Use the summary and keyword information to understand the information contained in the given database table secified in the information chunk.\n",
    "        ** Ensure that the query is as simple as possible. If the answer can be obtained from a single table, do not use JOINs. Select the most relevant table and form the query using only that table.**\n",
    "        ** Always add LIMIT 20 to the query if you see the SQL is SELECT query and User is not asking for a limited number of records. If user is asking for limited number of records, take a call to make the Limit to that number if it is lower than 20, else limit it to 20. If it is aggregation like Sum, COUNT etc., you dont need to add LIMIT clause. Think and apply limit wherever required. Do not add LIMIT unneccessarily. **\n",
    "        \n",
    "        Previous action outcome is: {previous_action_execution_result}\n",
    "\n",
    "        Information chunks starts here\n",
    "\n",
    "        {concatenated_content}\n",
    "\n",
    "        Information chunks ends here.\n",
    "\n",
    "        Action to be performed is: {action}\n",
    "\n",
    "        Use below template for output:\n",
    "\n",
    "        ```json\n",
    "        {{\n",
    "            \"query\": \"\"\n",
    "        }}\n",
    "        ```\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "UNSTR_ACTION_EXECUTOR = \"\"\"\n",
    "        You are a helpful assitant and your task is to execute a given action.\n",
    "        You have been provided outcomes of executions of previous actions. The previous actions outcome will be empty for the first action.\n",
    "        You have also been provided multiple information chunks to execute the current action.\n",
    "        Each information chunk starts with \"Information <number>:\" where <number> is the running number.\n",
    "        Each chunk also has a similarity score associated with it. It is marekd as \"Similarity: <score>\".\n",
    "        Always prefer to use the information chunk with highest similarity score to answer the question.\n",
    "        You may use all or just one of the information chunks to perform action.\n",
    "        To perform the action, you have to use the information chunks and extract a meaningful answer based on the contents of information chunks.\n",
    "        Do not use any information other than what is provided in the information chunk.\n",
    "        Use the summary and keyword information to understand the information contained in the information chunk.\n",
    "        \n",
    "        ** You must also take note of the Information numbers or chunks which were used to answer the question or perform action. You can include more than one Information number if multiple chunks were used.**\n",
    "\n",
    "\n",
    "        Previous action outcome is: {previous_action_execution_result}\n",
    "\n",
    "        Information chunks starts here\n",
    "\n",
    "        {concatenated_content}\n",
    "\n",
    "        Information chunks ends here.\n",
    "\n",
    "        Action to be performed is: {action}\n",
    "\n",
    "        Use below template for output:\n",
    "\n",
    "        ```\n",
    "        {{\n",
    "            \"answer\": \"\",\n",
    "            \"information_numbers\":[<1>,<2>,]\n",
    "        }}\n",
    "        ```\n",
    "        \"\"\"\n",
    "\n",
    "QUALITY_CHECKER = \"\"\"\n",
    "    You are a helpful assitant and your task is to check the quality of the execution result of a given action.\n",
    "    Check for completeness, correctness and relevance of the execution result against the given action.\n",
    "    If the execution result is complete, correct, relevant, then mark the quality as \"High\".\n",
    "    Otherwise, mark the quality as \"Low\".\n",
    "\n",
    "    **Action**: {action}\n",
    "\n",
    "    **Execution result**: {result_data}\n",
    "\n",
    "    Use below template for output:\n",
    "    \n",
    "    ```json\n",
    "    {{\n",
    "        \"quality\": \"\"\n",
    "    }}\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "ANSWER_SELECTOR = \"\"\"\n",
    "    Prompt: Selecting the Best Response\n",
    "Task: Given a question and a set of responses, identify the best response that comprehensively answers the question. The selected response should:\n",
    "\n",
    "Relevance: Directly address the question, focusing on the key aspects asked.\n",
    "Accuracy: Provide correct and factual information.\n",
    "Clarity: Be clear and easy to understand, avoiding ambiguity.\n",
    "Completeness: Cover all necessary points and details to provide a full answer.\n",
    "Conciseness: Be as brief as possible while being complete.\n",
    "Tone and Appropriateness: Maintain a respectful and appropriate tone.\n",
    "Instructions:\n",
    "\n",
    "Input: A question and a list of responses formatted as follows:\n",
    "question:{question}\n",
    "responses: {responses}\n",
    "Process:\n",
    "Evaluate each response based on the criteria listed above.\n",
    "Determine the response that best meets all the criteria.\n",
    "Output: A JSON object indicating the best response.\n",
    "\n",
    "```json\n",
    "    {{\n",
    "        \"best_response\": <Selected best answer>\n",
    "    }}\n",
    "    ```\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def answer_selector_prompt(question, answer_dict):\n",
    "    return f\"\"\"\n",
    "    You are an intelligent assistant tasked with finding the most relevant answer from a provided dictionary based on a specific question. The dictionary contains types of retrievers as keys and their corresponding answers as values.\n",
    "\n",
    "    Question: \"{question}\"\n",
    "\n",
    "    Answer Dictionary:\n",
    "    {{\n",
    "        {', '.join([f'\"{k}\": \"{v}\"' for k, v in answer_dict.items()])}\n",
    "    }}\n",
    "\n",
    "    Instructions:\n",
    "    1. Analyze each dictionary entry thoroughly.\n",
    "    2. Select the answer that most accurately and completely addresses the question.\n",
    "    3. Provide only the selected answer without any additional explanation or commentary.\n",
    "    4. If no answer in the dictionary adequately addresses the question, respond with \"No Answer Found\" and set relevance as False.\n",
    "\n",
    "    Response:\n",
    "    {{\n",
    "        \"key\": \"The key of the selected answer or 'dummy' if no suitable answer is found\",\n",
    "        \"response\": \"The selected answer or 'No Answer Found'\",\n",
    "        \"is_relevant\": true or false\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def final_answer_selector_prompt(keyword_question_emb, keyword_chunk_emb, entity_chunk_emb):\n",
    "    # Template for the prompt\n",
    "    prompt_template = \"\"\"  \n",
    "    You are a helpful assistant and your task is to check the quality of the execution result of a given action.\n",
    "   \n",
    "    Check for completeness, correctness and relevance of the execution result against the given question.\n",
    "    If the execution result is complete, correct, relevant, then mark that retriever as \"best_answer\".\n",
    " \n",
    "    The expected response format is:\n",
    " \n",
    "    {{\n",
    "        \"key\": \"The key of the JSON object (keyword_question_emb, keyword_chunk_emb, entity_chunk_emb) that provided the selected best answer, or 'dummy' if no suitable answer is found\",\n",
    "        \"reasoning\": \"Explain why this answer was chosen, considering the accuracy, relevance, and completeness of the actions. Ensure that the key matches the JSON object that provided this answer, and verify that the action types were appropriate for the actions.\"\n",
    "    }}\n",
    "    \n",
    "    ---\n",
    "\n",
    "    **Input JSONs:**\n",
    "\n",
    "    ```json\n",
    "    {{\n",
    "        \"keyword_question_emb\": {keyword_question_emb},\n",
    "        \"keyword_chunk_emb\": {keyword_chunk_emb},\n",
    "        \"entity_chunk_emb\": {entity_chunk_emb}\n",
    "    }}\n",
    "    ```\n",
    "\n",
    "    **Response:**\n",
    "\n",
    "    Please provide the response based on your evaluation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert JSON inputs to formatted strings\n",
    "    keyword_question_emb_str = json.dumps(keyword_question_emb, indent=4)\n",
    "    keyword_chunk_emb_str = json.dumps(keyword_chunk_emb, indent=4)\n",
    "    entity_chunk_emb_str = json.dumps(entity_chunk_emb, indent=4)\n",
    "\n",
    "    # Fill in the prompt template with the actual JSON content\n",
    "    prompt = prompt_template.format(\n",
    "        keyword_question_emb=keyword_question_emb_str,\n",
    "        keyword_chunk_emb=keyword_chunk_emb_str,\n",
    "        entity_chunk_emb=entity_chunk_emb_str\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "MULTI_DB_SELECTOR_PROMPT = \"\"\"\n",
    "You are an advanced language model tasked with analyzing database responses. Your objective is to determine whether these responses form a coherent answer collectively or if one response is the most accurate.\n",
    "\n",
    "Input Details: You will receive input in the form of a JSON object, where database names are the keys and their respective responses are the values.\n",
    "\n",
    "```json\n",
    "{input}\n",
    "```\n",
    "\n",
    "Output Requirements: Your response must be in JSON format using the following structure and here for the 'data' key below always provide a valid pandas dataframe.:\n",
    "\n",
    "```json\n",
    "{{\n",
    "  \"databases\": [],\n",
    "  \"queries\": [],\n",
    "  \"response\": \"\",\n",
    "  \"data\": pd.DataFrame,\n",
    "  \"reasoning\": \"\"\n",
    "}}\n",
    "```\n",
    "In the template here are the required fields' descriptions:\n",
    "\n",
    "databases: List the database names used to form the final response. If a single database was sufficient, list only that one; otherwise, list multiple databases.\n",
    "queries: Include the list of queries whose responses were used which can be single query if you used a single database or multiple queries if you used multiple databases.\n",
    "response: Provide a clear, concise, and synthesized answer based on the provided information, without adding any external details. It should clearly answer the question provided. If none of the answer/queries are relevant/correct to the answer, respond with \"No Answer Found\".\n",
    "data: If you selected a single database, include that database's data and format it to a valid pandas DataFrame. If multiple databases were used, combine the data accordingly to form a proper pandas dataframe combinedly. This value should be a valid pandas dataframe only.\n",
    "reasoning: Explain why you chose the particular database(s) and query response(s), justifying your selection. Also recheck if for the 'data' key, a valid pandas dataframe is provided or not.\n",
    "\n",
    "Process:\n",
    "\n",
    "1. Analyze the given responses.\n",
    "2. Decide whether to use a single response or synthesize multiple responses.\n",
    "3. For the 'response' key, provide a clear english language description of the answer. \n",
    "4. Format the output in the specified JSON template, providing clear reasoning for your choice.\n",
    "5. For the 'data' key always provide a valid pandas dataframe.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import re\n",
    "import json\n",
    "import socket\n",
    "import pymysql\n",
    "\n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=OPENAI_API_KEY,\n",
    ")\n",
    "\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "\n",
    "def openai_completion(prompt):\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=OPENAI_MODEL,\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant designed to output JSON.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "    \n",
    "\n",
    "def openai_completion_final_answer(prompt):\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=OPENAI_MODEL_4O,\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant designed to output JSON.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def extract_json_from_text(text):\n",
    "    pattern = re.compile(r\"```json(.*?)```\", re.DOTALL)\n",
    "    matches = pattern.findall(text)\n",
    "\n",
    "    extracted_json = []\n",
    "    for match in matches:\n",
    "        try:\n",
    "            json_data = json.loads(match.strip())\n",
    "            extracted_json.append(json_data)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "    return extracted_json\n",
    "\n",
    "\n",
    "def get_joined_keywords(keywords, seperator=\"&\"):\n",
    "    flattened_terms = [word for term in keywords for word in term.split()]\n",
    "    joined_keywords = seperator.join(flattened_terms)\n",
    "    return joined_keywords\n",
    "\n",
    "\n",
    "def get_document_level_dict(node_ids=[], rows=[]):\n",
    "    document_level_dict = {}\n",
    "\n",
    "    for node_id in node_ids:\n",
    "        for row in rows:\n",
    "            if row.get(\"id\") == node_id:\n",
    "                source_id = row.get(\"source_id\")\n",
    "                if source_id not in document_level_dict:\n",
    "                    document_level_dict[source_id] = []\n",
    "                document_level_dict[source_id].append(node_id)\n",
    "                document_level_dict[source_id] = list(set(document_level_dict[source_id]))\n",
    "\n",
    "    return document_level_dict\n",
    "\n",
    "\n",
    "def get_chunk_level_dict(response_node_ids, all_rows_content, word_list, retriever_flow):\n",
    "\n",
    "    chunk_dict = {}\n",
    "    neo4j_db = Neo4jDatabase(NEO4J_URL, NEO4J_USERNAME, NEO4J_PASSWORD)\n",
    "    neo4j_db.connect()\n",
    "    for node_id in response_node_ids:\n",
    "        if node_id not in chunk_dict:\n",
    "            chunk_dict[node_id] = []\n",
    "\n",
    "        node_details = next((node for node in all_rows_content if node[\"id\"] == node_id), None)\n",
    "        if node_details:\n",
    "            chunk_node_id = node_details.get(\"id\", \"\")\n",
    "\n",
    "            if retriever_flow == \"question\":\n",
    "                question_ids = neo4j_db.get_keyword_node_ids(\"\", chunk_node_id, retriever_flow)\n",
    "                if question_ids:\n",
    "                    if node_id not in chunk_dict:\n",
    "                        chunk_dict[node_id] = []\n",
    "                    chunk_dict[node_id] += question_ids\n",
    "                    chunk_dict[node_id] = list(set(chunk_dict[node_id]))\n",
    "\n",
    "                neo4j_db.close()\n",
    "                return chunk_dict\n",
    "\n",
    "            node_word_list = (\n",
    "                node_details.get(\"keywords\", [])\n",
    "                if retriever_flow in [\"keyword\", \"question\"]\n",
    "                else node_details.get(\"entities\", [])\n",
    "            )\n",
    "\n",
    "            print(\"+++++++ word list ++++++++\", word_list)\n",
    "            print(\"++++++ node word list ++++++++\", node_word_list)\n",
    "            for word in word_list:\n",
    "                print(\"+++++++++++++++++ word chunk_node_id  \", word, chunk_node_id)\n",
    "                if any(\n",
    "                    (word.lower() in node_word.lower()) or (node_word.lower() in word.lower())\n",
    "                    for node_word in node_word_list\n",
    "                ):\n",
    "                    word_id = neo4j_db.get_keyword_node_ids(word, chunk_node_id, retriever_flow)\n",
    "                    print(\n",
    "                        \"++++++++++++ matched  word+++++  \",\n",
    "                        word,\n",
    "                        \"   +++++matched with++++++++\",\n",
    "                        [node_word.lower() for node_word in node_word_list],\n",
    "                    )\n",
    "                    if word_id:\n",
    "                        if node_id not in chunk_dict:\n",
    "                            chunk_dict[node_id] = []\n",
    "                        chunk_dict[node_id] += word_id\n",
    "                        chunk_dict[node_id] = list(set(chunk_dict[node_id]))\n",
    "\n",
    "    neo4j_db.close()\n",
    "    return chunk_dict\n",
    "\n",
    "\n",
    "def get_simplified_json(input_data):\n",
    "    # Check if the input_data is empty\n",
    "    if not input_data:\n",
    "        return {\n",
    "            \"question\": \"No question found\",\n",
    "            \"actions\": {\n",
    "                \"action_1\": {\n",
    "                    \"current_action\": \"No actions found\",\n",
    "                    \"action_type\": None,\n",
    "                    \"sql\": None,\n",
    "                    \"answer\": \"No answer found\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "    simplified_json = {\n",
    "        \"question\": input_data.get(\"question\", \"No question found\"),\n",
    "        \"actions\": {}\n",
    "    }\n",
    "\n",
    "    # Handle actions if they exist, otherwise provide a default empty action\n",
    "    if \"actions\" in input_data:\n",
    "        for action_key, action_value in input_data[\"actions\"].items():\n",
    "            simplified_json[\"actions\"][action_key] = {\n",
    "                \"current_action\": action_value.get(\"current_action\", \"No current action\"),\n",
    "                \"action_type\": action_value.get(\"action_type\"),\n",
    "                \"sql\": action_value.get(\"sql\"),\n",
    "                \"answer\": action_value.get(\"answer\", \"No answer found\")\n",
    "            }\n",
    "    else:\n",
    "        simplified_json[\"actions\"][\"action_1\"] = {\n",
    "            \"current_action\": \"No actions found\",\n",
    "            \"action_type\": None,\n",
    "            \"sql\": None,\n",
    "            \"answer\": \"No answer found\"\n",
    "        }\n",
    "\n",
    "    return simplified_json\n",
    "\n",
    "def get_simplified_json_v2(input_data):\n",
    "    \n",
    "    # Configuration: Define properties to keep based on action_type\n",
    "    action_type_config = {\n",
    "        \"Structured\": [\"action_type\", \"sql\"],\n",
    "        \"Unstructured\": [\"current_action\", \"action_type\", \"answer\"]\n",
    "    }\n",
    "\n",
    "    # Check if the input_data is empty\n",
    "    if not input_data:\n",
    "        return {\n",
    "            \"question\": \"No question found\",\n",
    "            \"actions\": {\n",
    "                \"action_1\": {\n",
    "                    \"current_action\": \"No actions found\",\n",
    "                    \"action_type\": None,\n",
    "                    \"sql\": None,\n",
    "                    \"answer\": \"No answer found\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "    simplified_json = {\n",
    "        \"question\": input_data.get(\"question\", \"No question found\"),\n",
    "        \"actions\": {}\n",
    "    }\n",
    "\n",
    "    # Handle actions if they exist, otherwise provide a default empty action\n",
    "    if \"actions\" in input_data:\n",
    "        for action_key, action_value in input_data[\"actions\"].items():\n",
    "            action_type = action_value.get(\"action_type\")\n",
    "            action_dict = {}\n",
    "\n",
    "            if action_type in action_type_config:\n",
    "                for field in action_type_config[action_type]:\n",
    "                    action_dict[field] = action_value.get(field, f\"No {field} found\")\n",
    "            else:\n",
    "                # Default handling if action_type doesn't match any configured type\n",
    "                action_dict = {\n",
    "                    \"current_action\": action_value.get(\"current_action\", \"No current action\"),\n",
    "                    \"action_type\": action_value.get(\"action_type\"),\n",
    "                    \"sql\": action_value.get(\"sql\"),\n",
    "                    \"answer\": action_value.get(\"answer\", \"No answer found\"),\n",
    "                    \"table\": action_value.get(\"table\", None),\n",
    "                    \"csv\": action_value.get(\"csv\", None)\n",
    "                }\n",
    "\n",
    "            simplified_json[\"actions\"][action_key] = action_dict\n",
    "    else:\n",
    "        simplified_json[\"actions\"][\"action_1\"] = {\n",
    "            \"current_action\": \"No actions found\",\n",
    "            \"action_type\": None,\n",
    "            \"sql\": None,\n",
    "            \"answer\": \"No answer found\",\n",
    "            \"table\": None,\n",
    "            \"csv\": None\n",
    "        }\n",
    "\n",
    "    return simplified_json\n",
    "\n",
    "def extract_analysis_details(question_analysis):\n",
    "    \"\"\"Extract actions, keywords, information, and types from the question analysis.\"\"\"\n",
    "    actions = [action_dict[\"action\"] for action_dict in question_analysis[0]]\n",
    "    keywords = [action_dict[\"keywords\"] for action_dict in question_analysis[0]]\n",
    "    informations = [action_dict[\"information\"] for action_dict in question_analysis[0]]\n",
    "    types = [action_dict[\"type\"].lower() for action_dict in question_analysis[0]]\n",
    "    return actions, keywords, informations, types\n",
    "\n",
    "\n",
    "def combine_consecutive_actions(actions, keywords, types, informations):\n",
    "    \"\"\"Combine consecutive actions, keywords, and types that share the same type.\"\"\"\n",
    "    new_actions = []\n",
    "    new_keywords = []\n",
    "    new_types = []\n",
    "    new_information = []\n",
    "\n",
    "    i = 0\n",
    "    while i < len(actions):\n",
    "        combined_action = actions[i]\n",
    "        combined_type = types[i]\n",
    "        combined_keyword = keywords[i]\n",
    "        combined_information = informations[i]\n",
    "\n",
    "        while i + 1 < len(actions) and types[i] == types[i + 1]:\n",
    "            combined_action += \". \" + actions[i + 1]\n",
    "            combined_keyword += keywords[i + 1]\n",
    "            combined_information += informations[i + 1]\n",
    "            i += 1\n",
    "\n",
    "        new_actions.append(combined_action)\n",
    "        new_types.append(combined_type)\n",
    "        new_keywords.append(list(set(combined_keyword)))\n",
    "        new_information.append(combined_information)\n",
    "        i += 1\n",
    "\n",
    "    return new_actions, new_keywords, new_types, new_information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLM Completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple Database Inclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from typing import Optional\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class DatabaseType(str, Enum):\n",
    "    \"\"\"\n",
    "    Enum class for database types.\n",
    "    \"\"\"\n",
    "\n",
    "    MYSQL = \"mysql\"\n",
    "    PGSQL = \"postgres\"\n",
    "    SNOWFLAKE = \"snowflake\"\n",
    "    BIGQUERY = \"bigquery\"\n",
    "\n",
    "\n",
    "class ConnectionDetails(BaseModel):\n",
    "    \"\"\"\n",
    "    Pydantic model for connection details across different database types.\n",
    "\n",
    "    Attributes:\n",
    "        database_type (DatabaseType): The type of the database (e.g., PostgreSQL, MySQL, Snowflake).\n",
    "        host (Optional[str]): The host address of the database (if applicable).\n",
    "        port (Optional[str]): The port number for the database connection (if applicable).\n",
    "        user (Optional[str]): The username for the database connection (if applicable).\n",
    "        password (Optional[str]): The password for the database connection (if applicable).\n",
    "        database (Optional[str]): The name of the database (if applicable).\n",
    "        schema_name (Optional[str]): The schema name within the database (if applicable).\n",
    "        account (Optional[str]): The account identifier for the connection (specific to Snowflake).\n",
    "        role (Optional[str]): The role used for accessing the database (specific to certain databases).\n",
    "        warehouse (Optional[str]): The warehouse identifier (specific to Snowflake or other similar databases).\n",
    "        project_id (Optional[str]): The project ID (specific to cloud-based databases like Google BigQuery).\n",
    "        dataset (Optional[str]): The dataset name (specific to Google BigQuery).\n",
    "    \"\"\"\n",
    "\n",
    "    database_type: DatabaseType\n",
    "    host: Optional[str] = None\n",
    "    port: Optional[str] = None\n",
    "    user: Optional[str] = None\n",
    "    password: Optional[str] = None\n",
    "    database: Optional[str] = None\n",
    "    schema_name: Optional[str] = None\n",
    "    account: Optional[str] = None\n",
    "    role: Optional[str] = None\n",
    "    warehouse: Optional[str] = None\n",
    "    project_id: Optional[str] = None\n",
    "    dataset: Optional[str] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This module incorporates the necessary interface for ingesting different kinds of databases\"\"\"\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "BQ_SERVICE_ACCOUNT_JSON = {}\n",
    "\n",
    "\n",
    "class DatabaseInterface(ABC):\n",
    "    \"\"\"Generic class for database operations\"\"\"\n",
    "\n",
    "    # pylint: disable=E1101\n",
    "\n",
    "    @abstractmethod\n",
    "    def setup(self, details: ConnectionDetails, database_type: DatabaseType):\n",
    "        \"\"\"Function to setup the constructor\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def create_session_factory(self) -> sessionmaker:\n",
    "        \"\"\"\n",
    "        Creates a session factory using the provided database URL.\n",
    "        \"\"\"\n",
    "\n",
    "    def create_session(self) -> sessionmaker:\n",
    "        \"\"\"\n",
    "        Creates a SQLAlchemy session factory using the provided database URL.\n",
    "\n",
    "        Args:\n",
    "            database_url (str): The database URL.\n",
    "\n",
    "        Returns:\n",
    "            sessionmaker: A configured sessionmaker instance for creating database sessions.\n",
    "        \"\"\"\n",
    "        if self.database_url is None:\n",
    "            raise ValueError(\"Database URL is not set. Call setup() first.\")\n",
    "\n",
    "        if self.database_type == DatabaseType.BIGQUERY and BQ_SERVICE_ACCOUNT_JSON is not None:\n",
    "            engine = create_engine(self.database_url, credentials_path=BQ_SERVICE_ACCOUNT_JSON)\n",
    "        else:\n",
    "            engine = create_engine(self.database_url)\n",
    "\n",
    "        session_local = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n",
    "        return session_local\n",
    "\n",
    "    @abstractmethod\n",
    "    def test_connection(self) -> bool:\n",
    "        \"\"\"Functionality to test a connection\"\"\"\n",
    "\n",
    "    def get_session(self) -> sessionmaker:\n",
    "        \"\"\"\n",
    "        Provides a new session for database operations.\n",
    "        \"\"\"\n",
    "        if not self.session_factory:\n",
    "            self.create_session_factory()\n",
    "\n",
    "        return self.session_factory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This module provides a PostGres database connector implementation for the `DatabaseInterface`.\n",
    "It enables the creation of database sessions, establishes a connection to a PostGres database,\n",
    "and performs basic operations like testing the connection and checking schema existence.\n",
    "\"\"\"\n",
    "\n",
    "import traceback\n",
    "\n",
    "from fastapi import HTTPException\n",
    "from sqlalchemy import text\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "\n",
    "MAX_COMPARABLE_QUERY_STRING_LENGTH = 8\n",
    "\n",
    "# pylint: disable=R0902\n",
    "\n",
    "\n",
    "class PostGres(DatabaseInterface):\n",
    "    \"\"\"\n",
    "    Postgres class that implements the `DatabaseInterface` to connect to a PostGres database,\n",
    "    create sessions, and test connections.\n",
    "\n",
    "    Attributes:\n",
    "        database_type (str): Type of the database (PostGres).\n",
    "        host (str): Hostname or IP of the PostGres server.\n",
    "        port (int): Port number of the PostGres server.\n",
    "        user (str): Username to authenticate with the PostGres server.\n",
    "        password (str): Password to authenticate with the PostGres server.\n",
    "        database (str): Name of the database to connect to.\n",
    "        schema (str): Schema name to be checked for existence in the database.\n",
    "        session_factory (sessionmaker): SQLAlchemy session factory for handling DB connections.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, details: ConnectionDetails, database_type: DatabaseType):\n",
    "        \"\"\"\n",
    "        Initializes the PostGres class by setting up the connection details and configuring\n",
    "        the session factory.\n",
    "\n",
    "        Args:\n",
    "            details (ConnectionDetails): An object containing database connection details\n",
    "                                         (host, port, user, password, etc.).\n",
    "            database_type (DatabaseType): The type of the database (e.g., PostGres, PostgreSQL).\n",
    "        \"\"\"\n",
    "        self.session_factory = None\n",
    "        self.database_url = None  # Added to initialize the attribute\n",
    "        self.setup(details, database_type)\n",
    "\n",
    "    def setup(self, details: ConnectionDetails, database_type: DatabaseType):\n",
    "        \"\"\"\n",
    "        Configures the PostGres connection settings, ensuring that all necessary fields\n",
    "        (host, port, user, password, database, and schema_name) are provided.\n",
    "\n",
    "        Args:\n",
    "            details (ConnectionDetails): Connection details such as host, port, user, password,\n",
    "                                         database, and schema_name.\n",
    "            database_type (DatabaseType): The type of the database (used to differentiate PostGres from others).\n",
    "\n",
    "        Raises:\n",
    "            HTTPException: If any of the required fields (host, port, user, password, database, or schema_name)\n",
    "                           are missing, an HTTP 400 exception is raised.\n",
    "        \"\"\"\n",
    "        if details.host is None:\n",
    "            print(\"Host is required and missing\")\n",
    "            raise HTTPException(status_code=400, detail=\"Host is required and missing\")\n",
    "\n",
    "        if details.port is None:\n",
    "            print(\"Port is required and missing\\n\", traceback.format_exc())\n",
    "            raise HTTPException(status_code=400, detail=\"Port is required and missing\")\n",
    "\n",
    "        if details.user is None:\n",
    "            print(\"User is required and missing!!\\n\", traceback.format_exc())\n",
    "            raise HTTPException(status_code=400, detail=\"User is required and missing\")\n",
    "\n",
    "        if details.database is None:\n",
    "            print(\"Database is required and missing\\n\", traceback.format_exc())\n",
    "            raise HTTPException(\n",
    "                status_code=400,\n",
    "                detail=\"Database is required and missing for the connection\",\n",
    "            )\n",
    "\n",
    "        if details.password is None:\n",
    "            print(\"Password is required and missing\\n\", traceback.format_exc())\n",
    "            raise HTTPException(\n",
    "                status_code=400,\n",
    "                detail=\"Password is required and missing for the connection\",\n",
    "            )\n",
    "\n",
    "        if details.schema_name is None:\n",
    "            print(\"Schema name is required and missing\\n\", traceback.format_exc())\n",
    "            raise HTTPException(status_code=400, detail=\"Schema name is required and missing\")\n",
    "\n",
    "        self.database_type = DatabaseType.PGSQL\n",
    "        self.host = details.host\n",
    "        self.port = details.port\n",
    "        self.user = details.user if details.user else None\n",
    "        self.password = details.password\n",
    "        self.database = details.database\n",
    "        self.schema = details.schema_name if details.schema_name else None\n",
    "\n",
    "\n",
    "    def create_session_factory(self) -> sessionmaker:\n",
    "        \"\"\"\n",
    "        Creates and returns an Postgres\n",
    "        SQLAlchemy session factory for\n",
    "        establishing connections to the\n",
    "        PostGres database.\n",
    "        It builds the PostGres connection URL based on the\n",
    "        provided connection details.\n",
    "\n",
    "        Returns:\n",
    "            sessionmaker: SQLAlchemy session factory for\n",
    "            managing database connections.\n",
    "\n",
    "        Raises:\n",
    "            None: The method prints an error message and returns `None` if the database URL is invalid.\n",
    "        \"\"\"\n",
    "        self.database_url = f\"postgresql://{self.user}:{self.password}@{self.host}:{self.port}/{self.database}\"\n",
    "\n",
    "        try:\n",
    "            self.session_factory = self.create_session()\n",
    "        except Exception as error:\n",
    "            print(f\"Invalid postgres database connection!!-{error}\\n\", traceback.format_exc())\n",
    "            raise HTTPException(status_code=401, detail=f\"Invalid database connection!!-{error}\") from error\n",
    "\n",
    "        return self.session_factory\n",
    "\n",
    "    def test_connection(self) -> bool:\n",
    "        \"\"\"\n",
    "        Tests the connection to the PostGres\n",
    "        database by executing a simple query and\n",
    "        checking the existence of\n",
    "        the specified schema in the database.\n",
    "\n",
    "        The method performs two main actions:\n",
    "        1. Executes a simple `SELECT 1` query to\n",
    "        verify the connection.\n",
    "        2. Checks if the specified schema exists\n",
    "        in the PostGres database by querying the\n",
    "        `INFORMATION_SCHEMA.SCHEMATA` table.\n",
    "\n",
    "        Returns:\n",
    "            bool: Returns `True` if the connection is\n",
    "            successful and the schema exists, `False`\n",
    "            otherwise.\n",
    "\n",
    "        Raises:\n",
    "            HTTPException: If the schema does not exist,\n",
    "            an HTTP 400 exception is raised.\n",
    "        \"\"\"\n",
    "        query = \"SELECT 2\"\n",
    "        check_schema_query = \"\"\"\n",
    "        SELECT schema_name \n",
    "        FROM information_schema.schemata \n",
    "        WHERE schema_name = :schema_name\n",
    "        \"\"\"\n",
    "        if not self.session_factory:\n",
    "            self.create_session_factory()\n",
    "\n",
    "        try:\n",
    "            with self.session_factory() as session:\n",
    "                # Test the connection\n",
    "                session.execute(text(query))\n",
    "\n",
    "                # Check if schema exists in PostGres\n",
    "                result = session.execute(text(check_schema_query), {\"schema_name\": self.schema}).fetchone()\n",
    "\n",
    "                if not result:\n",
    "                    raise HTTPException(\n",
    "                        status_code=400,\n",
    "                        detail=f\"Schema '{self.schema}' does not exist in the database\",\n",
    "                    )\n",
    "\n",
    "                print(\"Connection Successful!!\")\n",
    "                return True\n",
    "\n",
    "        except Exception as error:\n",
    "            print(f\"Invalid postgres database connection!!-{error}\\n\", traceback.format_exc())\n",
    "            raise HTTPException(status_code=401, detail=f\"Invalid database connection!!-{error}\") from error\n",
    "\n",
    "    def cancel_job_by_query(self, query_string: str):\n",
    "        \"\"\"\n",
    "        Kill queries in PostgreSQL that match a given query string.\n",
    "\n",
    "        Args:\n",
    "            query_string (str): The query string to match against running queries.\n",
    "        \"\"\"\n",
    "        if not self.session_factory:\n",
    "            self.create_session_factory()\n",
    "\n",
    "        try:\n",
    "            with self.session_factory() as session:\n",
    "                # Step 1: Get the list of active queries from pg_stat_activity\n",
    "                query = \"\"\"\n",
    "                    SELECT pid, usename, state, query, query_start\n",
    "                    FROM pg_stat_activity\n",
    "                    WHERE state = 'active';\n",
    "                \"\"\"\n",
    "                result = session.execute(text(query))\n",
    "                active_queries = result.fetchall()\n",
    "\n",
    "                print(f\"Active queries: {active_queries}\")\n",
    "\n",
    "                # Step 2: Iterate over the active queries and kill matching ones\n",
    "                for query_info in active_queries:\n",
    "                    # Extract the query and process information\n",
    "                    pid = query_info[0]\n",
    "                    active_query = query_info[3]\n",
    "\n",
    "                    # Log the query info for debugging\n",
    "                    print(f\"Process ID: {pid}, Query: {active_query}\")\n",
    "\n",
    "                    # Step 3: Check if the active query matches the provided query_string\n",
    "                    if (\n",
    "                        active_query\n",
    "                        and active_query[:MAX_COMPARABLE_QUERY_STRING_LENGTH]\n",
    "                        == query_string[:MAX_COMPARABLE_QUERY_STRING_LENGTH]\n",
    "                    ):\n",
    "                        print(f\"Killing process {pid} for query: {active_query}\")\n",
    "\n",
    "                        # Step 4: Terminate the query by its process ID (pid)\n",
    "                        kill_query = f\"SELECT pg_terminate_backend({pid});\"\n",
    "                        session.execute(text(kill_query))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error killing queries: {e}\\n\", traceback.format_exc())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json_response_openai(\n",
    "    prompt: str,\n",
    "    model_name: str = OPENAI_MODEL,\n",
    "    temperature: int = 0,\n",
    "    max_tokens: int = 4000,\n",
    "    messages: list = None,\n",
    "    response_format: dict = None,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Sends a prompt to OpenAI's API and retrieves the response in JSON format.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The prompt text to send to the OpenAI API.\n",
    "        model_name (str): Specifies the model used for generating responses. Defaults to the\n",
    "                          global `OPENAI_MODEL`.\n",
    "        temperature (int): Adjusts randomness in the generated response. Lower values make it\n",
    "                           more deterministic. Defaults to 0.\n",
    "        max_tokens (int): The maximum number of tokens in the generated response. Defaults to 4000.\n",
    "        messages (list): A list of conversational messages sent to the OpenAI API. If not provided,\n",
    "                         the function wraps the `prompt` as a user message.\n",
    "        response_format (dict): A dictionary defining the expected response format. If not provided,\n",
    "                                defaults to `{\"type\": \"json_object\"}`.\n",
    "\n",
    "    Returns:\n",
    "        dict: The parsed JSON response from OpenAI.\n",
    "\n",
    "    Raises:\n",
    "        json.JSONDecodeError: If the response cannot be parsed into valid JSON.\n",
    "    \"\"\"\n",
    "    # Initialize messages if not provided\n",
    "    messages_default = messages or [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    # Set default response format if not provided\n",
    "    response_default = response_format or {\"type\": \"json_object\"}\n",
    "\n",
    "    tokens = max_tokens\n",
    "\n",
    "    # Send the request to OpenAI API and retrieve the response\n",
    "    openai_response = (\n",
    "        client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            temperature=temperature,\n",
    "            max_tokens=tokens,\n",
    "            messages=messages_default,\n",
    "            response_format=response_default,\n",
    "        )\n",
    "        .choices[0]\n",
    "        .message.content\n",
    "    )\n",
    "\n",
    "    # Parse the OpenAI response content into JSON\n",
    "    json_response = json.loads(openai_response)\n",
    "    return json_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import traceback\n",
    "import pandas as pd\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=OPENAI_API_KEY,\n",
    ")\n",
    "\n",
    "empty_cancel_response = {\n",
    "    \"sql\": \"\",\n",
    "    \"csv\": \"\",\n",
    "    \"table\": None,\n",
    "    \"query_resultset\": None,\n",
    "    \"answer\": \"\",\n",
    "    \"node_ids_used\": [],\n",
    "}\n",
    "\n",
    "\n",
    "def extract_table_names(query):\n",
    "    regex = r\"(?:FROM|JOIN)\\s+((?:\\w+\\.)?\\`?\\w+\\`?)\\s*(?:AS\\s+\\w+\\s*)?\"\n",
    "    matches = re.findall(regex, query, re.IGNORECASE)\n",
    "\n",
    "    # Remove schema prefix if present and duplicates\n",
    "    table_names = set(match.split(\".\")[-1].strip(\"`\") for match in matches)\n",
    "\n",
    "    return list(table_names)\n",
    "\n",
    "\n",
    "def get_keywords_linkedq_typeinfo_from_question(question, prompt):\n",
    "    print(f\"Question: {question}\\n\")\n",
    "\n",
    "    response = (\n",
    "        client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,\n",
    "                }\n",
    "            ],\n",
    "            temperature=0,\n",
    "            model=OPENAI_MULTI_MODEL,\n",
    "            max_tokens=4000,\n",
    "        )\n",
    "        .choices[0]\n",
    "        .message.content\n",
    "    )\n",
    "\n",
    "    json_response = extract_json_from_text(response)\n",
    "\n",
    "    print(f\"Analysis:\\n{json.dumps(json_response, indent=2)}\")\n",
    "    return json_response\n",
    "\n",
    "\n",
    "def concatenate_content(rows_content, content_type):\n",
    "    concatenated_content = \"\"\n",
    "    node_ids_used = []\n",
    "    counter = 1\n",
    "    for row_dict in rows_content:\n",
    "        if row_dict.get(\"type\") == content_type:\n",
    "            chunk_content = row_dict.get(\"chunk_content\", \"\")\n",
    "            if chunk_content:  # Only add if chunk_content is not empty\n",
    "                if content_type == \"unstructured\":\n",
    "                    summary = row_dict.get(\"summary\", \"\")\n",
    "                    keywords_combined = row_dict.get(\"keywords_combined\", \"\")\n",
    "                    chunk_content = f\"{chunk_content}\\nSummary: {summary}\\nKeywords: {keywords_combined}\\n\"\n",
    "                node_ids_used.append(row_dict.get(\"id\", \"\"))\n",
    "                concatenated_content += f\"Information {counter}: {chunk_content}\\nSimilarity: {row_dict.get('similarity', 0)}\\n\"\n",
    "                counter += 1\n",
    "    return concatenated_content, node_ids_used\n",
    "\n",
    "\n",
    "def create_prompt(\n",
    "    action: str,\n",
    "    concatenated_content: str,\n",
    "    previous_action_execution_result: str,\n",
    "    prompt_template: str,\n",
    "    database_type: str = None,\n",
    "    additional_detail: str = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Create a prompt by filling a template with the provided details.\n",
    "\n",
    "    Args:\n",
    "        action (str): The action to perform.\n",
    "        concatenated_content (str): The concatenated content for the prompt.\n",
    "        previous_action_execution_result (str): The result of the previous action execution.\n",
    "        prompt_template (str): The template to use for the prompt.\n",
    "\n",
    "    Returns:\n",
    "        str: The filled prompt ready for processing.\n",
    "    \"\"\"\n",
    "    if database_type:\n",
    "        return prompt_template.format(\n",
    "            action=action,\n",
    "            concatenated_content=concatenated_content,\n",
    "            previous_action_execution_result=previous_action_execution_result,\n",
    "            database_type=database_type,\n",
    "            additional_detail_string=additional_detail,\n",
    "        )\n",
    "\n",
    "    return prompt_template.format(\n",
    "        action=action,\n",
    "        concatenated_content=concatenated_content,\n",
    "        previous_action_execution_result=previous_action_execution_result,\n",
    "    )\n",
    "\n",
    "\n",
    "def execute_openai_prompt(prompt, model):\n",
    "    print(\n",
    "        \"############################################################################################################\"\n",
    "    )\n",
    "    print(f\"Prompt:\\n{prompt}\")\n",
    "    print(\n",
    "        \"############################################################################################################\"\n",
    "    )\n",
    "\n",
    "    response = (\n",
    "        client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,\n",
    "                }\n",
    "            ],\n",
    "            temperature=0,\n",
    "            model=model,\n",
    "            max_tokens=4000,\n",
    "        )\n",
    "        .choices[0]\n",
    "        .message.content\n",
    "    )\n",
    "\n",
    "    json_response = extract_json_from_text(response)\n",
    "    return json_response\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def extract_ddl_related_for_json_list(json_list: List[Dict[str, Any]]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extracts DDL statements from the 'chunk_content' field in a list of JSON objects.\n",
    "    Looks for content between \"DDL:\" and \"Related Tables:\".\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    json_list : List[Dict[str, Any]]\n",
    "        A list of dictionaries containing 'chunk_content' strings.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    List[str]\n",
    "        A list of extracted DDL statements.\n",
    "    \"\"\"\n",
    "    pattern = re.compile(r\"DDL:(.*?)Related Tables:\", re.DOTALL)\n",
    "    extracted_ddls = []\n",
    "\n",
    "    for item in json_list:\n",
    "        chunk_content = item.get(\"chunk_content\", \"\")\n",
    "        match = pattern.search(chunk_content)\n",
    "        if match:\n",
    "            extracted_ddls.append(match.group(1).strip())\n",
    "\n",
    "    return extracted_ddls\n",
    "\n",
    "def process_query(\n",
    "    connection_id,\n",
    "    value,\n",
    "    user_sessions,\n",
    "    run_id,\n",
    "    result_queue,\n",
    "    topic,\n",
    "    rows_content: list,\n",
    "    action: str,\n",
    "):\n",
    "    \"\"\"\n",
    "    Process a single query, execute it, and store the result in the result queue.\n",
    "    \"\"\"\n",
    "\n",
    "    query = value[-1]\n",
    "    connection_details = value[0][0]\n",
    "\n",
    "    if query is None or len(query) == 0 or query == \"\":\n",
    "        return\n",
    "\n",
    "    print(f\"Generated SQL: {query}\")\n",
    "\n",
    "    tables = extract_table_names(query)\n",
    "    print(f\"Extracted Tables SQL from query {query}: {tables}\")\n",
    "\n",
    "    chunks_to_select = [row for row in rows_content if row[\"table_name\"] in tables]\n",
    "    ddls_list = extract_ddl_related_for_json_list(chunks_to_select)\n",
    "\n",
    "    print(\"Starting to execute generated SQL query...\")\n",
    "\n",
    "    max_retries = int(os.getenv(\"RETRIEVAL_MAX_RETRIES\", \"2\"))\n",
    "    correct_flag = False\n",
    "    query_json = {}\n",
    "\n",
    "    for _ in range(max_retries):\n",
    "\n",
    "        try:\n",
    "            # Setup and execute the query\n",
    "            postgres_details = ConnectionDetails(  \n",
    "                database_type=\"postgres\",\n",
    "                host=PG_HOST,\n",
    "                port=PG_PORT,\n",
    "                user=PG_USER,\n",
    "                password=PG_PASSWORD,\n",
    "                database=PG_DATABASE,\n",
    "                schema_name=PG_SCHEMA,\n",
    "            )\n",
    "            database_instance = PostGres(postgres_details, postgres_details.database_type)\n",
    "            database_instance.create_session_factory()\n",
    "            session_factory = database_instance.session_factory()\n",
    "\n",
    "            print(\"Starting to execute generated SQL query...\")\n",
    "\n",
    "            query_response = execute_query(query, connection_details, user_sessions=user_sessions, run_id=run_id)\n",
    "\n",
    "            query_json = json.loads(query_response)\n",
    "            correct_flag = True\n",
    "            break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Exception while executing query: {e}\\n\", traceback.format_exc())\n",
    "            query = refiner(\n",
    "                query=action,\n",
    "                sql=query,\n",
    "                error=str(e),\n",
    "                exception_class=e.__class__.__name__,\n",
    "                tables=ddls_list,\n",
    "            )\n",
    "            continue\n",
    "\n",
    "    if not correct_flag:\n",
    "        query_response = json.dumps({\"message\": \"An error occurred.\", \"data\": []})\n",
    "    result_queue.put((connection_id, query_json))\n",
    "\n",
    "\n",
    "\n",
    "def execute_queries_concurrently(chunk_connection, user_sessions, run_id, topic, rows_content, action):\n",
    "    \"\"\"\n",
    "    Execute all queries concurrently using multiprocessing.\n",
    "    \"\"\"\n",
    "    result_queue = multiprocessing.Queue()\n",
    "    processes = []\n",
    "\n",
    "    for connection_id, value in chunk_connection.items():\n",
    "        process = multiprocessing.Process(\n",
    "            target=process_query,\n",
    "            args=(connection_id, value, user_sessions, run_id, result_queue, topic, rows_content, action),\n",
    "        )\n",
    "        processes.append(process)\n",
    "        process.start()\n",
    "\n",
    "    for process in processes:\n",
    "        process.join()\n",
    "\n",
    "    # Collect results from the queue and update chunk_connection\n",
    "    while not result_queue.empty():\n",
    "        connection_id, query_response = result_queue.get()\n",
    "        chunk_connection[connection_id].append(query_response)\n",
    "\n",
    "    return chunk_connection\n",
    "\n",
    "def database_password_decrypt(encrypted_message: str) -> str:\n",
    "    \"\"\"Decrypt an encrypted database password using AES.\"\"\"\n",
    "    key = os.getenv(\"DATABASE_PASSWORD_KEY\")\n",
    "    # Ensure the key is converted to bytes and is 16 bytes long for AES-128\n",
    "    key_bytes = key.encode(\"utf-8\")\n",
    "    if len(key_bytes) != 16:\n",
    "        raise ValueError(\"Key must be 16 bytes long for AES-128\")\n",
    "\n",
    "    # Decode the base64 encoded message\n",
    "    encrypted_data = base64.b64decode(encrypted_message)\n",
    "\n",
    "    # Extract the IV from the encrypted data\n",
    "    iv = encrypted_data[:16]\n",
    "    encrypted_data = encrypted_data[16:]\n",
    "\n",
    "    # Create AES cipher object\n",
    "    cipher = AES.new(key_bytes, AES.MODE_CBC, iv)\n",
    "\n",
    "    # Decrypt the data\n",
    "    decrypted_padded_data = cipher.decrypt(encrypted_data)\n",
    "\n",
    "    # Unpad the data to get the original plaintext\n",
    "    decrypted_data = unpad(decrypted_padded_data, AES.block_size)\n",
    "\n",
    "    # Convert the decrypted bytes back to a string\n",
    "    text = decrypted_data.decode(\"utf-8\")\n",
    "\n",
    "    return text\n",
    "\n",
    "def is_valid_base64(s: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if a given string is a valid Base64 encoded string.\n",
    "\n",
    "    This function attempts to decode the provided string using Base64. If the decoding\n",
    "    is successful, it returns True, indicating that the string is a valid Base64 encoded string.\n",
    "    If the decoding fails (due to incorrect padding or invalid characters), it returns False.\n",
    "\n",
    "    Args:\n",
    "        s (str): The string to be checked.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the string is a valid Base64 encoded string, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Attempt to decode the string from Base64\n",
    "        base64.b64decode(s)\n",
    "        return True\n",
    "    except base64.binascii.Error:\n",
    "        # If decoding fails, return False\n",
    "        return False\n",
    "\n",
    "def execute_structured_query(\n",
    "    action: str,\n",
    "    rows_content: list,\n",
    "    previous_action_execution_result: str,\n",
    "    str_prompt_template: str,\n",
    "    user_sessions: list,\n",
    "    run_id: str = None,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Execute a structured query and process the response.\n",
    "\n",
    "    Args:\n",
    "        action (str): The action to execute.\n",
    "        rows_content (list): The content of the rows to be processed.\n",
    "        previous_action_execution_result (str): The result of the previous action execution.\n",
    "        str_prompt_template (str): The template for the structured query prompt.\n",
    "        user_sessions (list): The user session details.\n",
    "        run_id (str, optional): The ID of the current run. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        dict: The final response containing SQL, CSV, table, query result set, answer, and node IDs used.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        print(\"chunk content:\", rows_content)\n",
    "        chunks = rows_content\n",
    "\n",
    "        chunk_connection = defaultdict(list)\n",
    "\n",
    "        for chunk in chunks:\n",
    "            connection_id = chunk[\"source_id\"]\n",
    "            value = []\n",
    "\n",
    "            detailed_connection_received = find_connection_details(connection_id)\n",
    "\n",
    "            connection_details = ConnectionDetails(\n",
    "            database_type=detailed_connection_received[1],\n",
    "            host=detailed_connection_received[2],\n",
    "            port=str(detailed_connection_received[3]),\n",
    "            user=detailed_connection_received[4],\n",
    "            password=(\n",
    "                    database_password_decrypt(detailed_connection_received[5])\n",
    "                    if is_valid_base64(detailed_connection_received[5])\n",
    "                    else detailed_connection_received[5]\n",
    "                ),\n",
    "            database=detailed_connection_received[6],\n",
    "            schema_name=str(detailed_connection_received[7])\n",
    "            )\n",
    "\n",
    "            # connection_details = ConnectionDetails(\n",
    "            #     database_type=detailed_connection_received[1],\n",
    "            #     host=detailed_connection_received[2],\n",
    "            #     port=str(detailed_connection_received[3]),\n",
    "            #     user=detailed_connection_received[4],\n",
    "            #     password=(\n",
    "            #         database_password_decrypt(detailed_connection_received[5])\n",
    "            #         if is_valid_base64(detailed_connection_received[5])\n",
    "            #         else detailed_connection_received[5]\n",
    "            #     ),\n",
    "            #     database=detailed_connection_received[6],\n",
    "            #     schema_name=detailed_connection_received[7]\n",
    "            # )\n",
    "            value.append(connection_details)\n",
    "\n",
    "            value.append(chunk)\n",
    "\n",
    "            chunk_connection[connection_id].append(value)\n",
    "        \n",
    "        print(\"Value ++++++++++++++++: \" , chunk_connection)\n",
    "\n",
    "        for connection_id, value in chunk_connection.items():\n",
    "\n",
    "            connection_details = value[0][0]\n",
    "\n",
    "            rows_content = [item[1] for item in value]\n",
    "\n",
    "            concatenated_content, _ = concatenate_content(rows_content, \"structured\")\n",
    "            additional_detail = \"\"\n",
    "            if connection_details.database_type == DatabaseType.BIGQUERY:\n",
    "                additional_detail = (\n",
    "                    f\"with project id {connection_details.database} and dataset name {connection_details.schema_name}\"\n",
    "                )\n",
    "            elif connection_details.database_type == DatabaseType.PGSQL:\n",
    "                additional_detail = f\"with schema name {connection_details.schema_name}\"\n",
    "            elif connection_details.database_type == DatabaseType.SNOWFLAKE:\n",
    "                additional_detail = f\"with schema name {connection_details.schema_name}\"\n",
    "\n",
    "            prompt = create_prompt(\n",
    "                action,\n",
    "                concatenated_content,\n",
    "                previous_action_execution_result,\n",
    "                str_prompt_template,\n",
    "                connection_details.database_type,\n",
    "                additional_detail,\n",
    "            )\n",
    "\n",
    "            json_response = execute_openai_prompt(model=OPENAI_MULTI_MODEL, prompt=prompt)\n",
    "            print(\n",
    "                    f\"JSON response by gemini from function execute_structured_query: {json_response}\"\n",
    "                )\n",
    "\n",
    "            query = json_response[0][\"query\"]\n",
    "            # pylint: disable=R1733\n",
    "            chunk_connection[connection_id].append(query)\n",
    "\n",
    "    # pylint: disable=W0718\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f\"Error while extracting connection details: {e}\"\n",
    "        )\n",
    "\n",
    "\n",
    "    result = {\"cancelled\": False}\n",
    "\n",
    "    # cancellation_thread = threading.Thread(\n",
    "    #     target=listen_for_cancellation, args=(run_id, result, query, connection_details)\n",
    "    # )\n",
    "    # cancellation_thread.daemon = True\n",
    "    # cancellation_thread.start()\n",
    "\n",
    "    chunk_connection = execute_queries_concurrently(\n",
    "        chunk_connection=chunk_connection,\n",
    "        user_sessions=user_sessions,\n",
    "        run_id=run_id,\n",
    "        topic=\"topic\",\n",
    "        rows_content=rows_content,\n",
    "        action=action,\n",
    "    )\n",
    "    print(\n",
    "        \"Executed queries concurrently. Results: {chunk_connection}\"\n",
    "    )\n",
    "\n",
    "    connection_response = defaultdict(list)\n",
    "    prompt_input = f\"Question: {action}\\n\"\n",
    "    for connection_id, value in chunk_connection.items():\n",
    "        query_response = value[-1]\n",
    "        query = value[-2]\n",
    "        connection_details = value[0][0]\n",
    "        if query is None or len(query) == 0:\n",
    "            continue\n",
    "\n",
    "        connection_response[connection_id].append(connection_details)\n",
    "        connection_response[connection_id].append(query_response)\n",
    "\n",
    "        prompt_input += f\"Database Name: {connection_details.schema_name}\\n\"\n",
    "        prompt_input += f\"SQL Query Used: {query}\\n\"\n",
    "        prompt_input += f\"Response: {query_response}\\n\\n\"\n",
    "\n",
    "\n",
    "    # str_response_prompt = STR_RESPONSE_SYNTHESIZER.format(\n",
    "    #     question=action, sql=json_response[0][\"query\"], response=query_response\n",
    "    # )\n",
    "    # if db_handler.check_run_cancelled(run_id=run_id):\n",
    "    #     return empty_cancel_response_structured\n",
    "\n",
    "    # # str_response = openai_completion(str_response_prompt)\n",
    "    # messages = [\n",
    "    #     {\n",
    "    #         \"role\": \"system\",\n",
    "    #         \"content\": \"You are a helpful assistant designed to output JSON.\",\n",
    "    #     },\n",
    "    #     {\"role\": \"user\", \"content\": str_response_prompt},\n",
    "    # ]\n",
    "    # if os.getenv(\"CONSTANT_LLM_MODEL\") == GEMINI_MODEL:\n",
    "    #     str_response = get_json_response_gemini\n",
    "    # (prompt=str_response_prompt, model_name=GEMINI_MODEL, messages=messages)\n",
    "    # else:\n",
    "    #     str_response = get_json_response_openai\n",
    "    # (prompt=str_response_prompt, model_name=OPENAI_MODEL, messages=messages)\n",
    "\n",
    "    str_response_prompt = MULTI_DB_SELECTOR_PROMPT.format(input=prompt_input)\n",
    "\n",
    "    # str_response = openai_completion(str_response_prompt)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant designed to output JSON.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": str_response_prompt},\n",
    "    ]\n",
    "\n",
    "    str_response = get_json_response_openai(prompt=str_response_prompt, model_name=OPENAI_MODEL, messages=messages)\n",
    "\n",
    "\n",
    "    chunks_to_select = []\n",
    "    tables = []\n",
    "    sql = \"\\n\"\n",
    "    count = 1\n",
    "    for query in str_response[\"queries\"]:\n",
    "        sql += f\"Query {count}: {query}\\n\"\n",
    "        connection_details = value[0][0]\n",
    "        table = extract_table_names(query)\n",
    "        tables.extend(table)\n",
    "        count += 1\n",
    "\n",
    "    if len(str_response[\"queries\"]) == 1:\n",
    "        sql = str_response[\"queries\"][0]\n",
    "\n",
    "    chunks_to_select = [row for row in chunks if row[\"table_name\"] in tables]\n",
    "\n",
    "    df = pd.DataFrame(str_response[\"data\"])\n",
    "    file_url = \"\"\n",
    "    data = \"\"\n",
    "\n",
    "    num_rows = df.shape[0]\n",
    "    num_columns = df.shape[1]\n",
    "\n",
    "    if num_rows > 1 or num_columns > 2:\n",
    "        file_url = \"\"\n",
    "        df = df.loc[:, ~df.columns.duplicated()]\n",
    "        data = json.dumps(df.to_json(orient=\"records\", date_format=\"iso\"))\n",
    "\n",
    "    final_response = {\n",
    "        \"sql\": sql,\n",
    "        \"csv\": file_url,\n",
    "        \"table\": data,\n",
    "        \"query_resultset\": str_response,\n",
    "        \"answer\": str_response[\"response\"],\n",
    "        \"node_ids_used\": [node[\"id\"] for node in chunks_to_select],\n",
    "    }\n",
    "    return final_response\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rest Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def execute_structured_query_2(\n",
    "    action,\n",
    "    rows_content,\n",
    "    previous_action_execution_result,\n",
    "    str_prompt_template,\n",
    "    user_sessions,\n",
    "    run_id=None,\n",
    "):\n",
    "    concatenated_content, _ = concatenate_content(rows_content, \"structured\")\n",
    "    prompt = create_prompt(\n",
    "        action,\n",
    "        concatenated_content,\n",
    "        previous_action_execution_result,\n",
    "        str_prompt_template,\n",
    "    )\n",
    "    json_response = execute_openai_prompt(prompt, OPENAI_MODEL_4O)\n",
    "    # message = f\"Str Action result (before execution query) from LLM:\\n{json.dumps(json_response, indent=2)}\"\n",
    "    # print(message)\n",
    "\n",
    "    # publish_message(\n",
    "    #     publisher=publisher,\n",
    "    #     event_type=EventType.RETRIEVER,\n",
    "    #     user_sessions=user_sessions,\n",
    "    #     message=message,\n",
    "    #     event_data={},\n",
    "    # )\n",
    "\n",
    "    query = json_response[0][\"query\"]\n",
    "    message = f\"Generated SQL: {query}\"\n",
    "    print(message)\n",
    "    tables = extract_table_names(query)\n",
    "    message = f\"Extracted Tables SQL: {tables}\"\n",
    "\n",
    "    chunks_to_select = []\n",
    "\n",
    "    for row in rows_content:\n",
    "        if row[\"table_name\"] in tables:\n",
    "            chunks_to_select.append(row)\n",
    "\n",
    "    chunks_to_select = chunks_to_select\n",
    "    connection_id = (\n",
    "        chunks_to_select[0][\"source_id\"]\n",
    "        if len(chunks_to_select) > 0\n",
    "        else rows_content[0][\"source_id\"]\n",
    "    )\n",
    "\n",
    "    connection_received = find_connection_details(connection_id)\n",
    "    print(\"*****************************\")\n",
    "    print(tables)\n",
    "    connection_details = ConnectionDetails(\n",
    "        database_type=connection_received[1],\n",
    "        host=connection_received[2],\n",
    "        port=connection_received[3],\n",
    "        user=connection_received[4],\n",
    "        password=connection_received[5],\n",
    "        database=connection_received[6],\n",
    "    )\n",
    "\n",
    "\n",
    "    max_retries = RETRIEVAL_MAX_RETRIES\n",
    "    correct_flag = False\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            query_response = execute_query(\n",
    "                query, connection_details, user_sessions=user_sessions, run_id=run_id\n",
    "            )\n",
    "            query_json = json.loads(query_response)\n",
    "\n",
    "            print(f\"******{query_json}\")\n",
    "            # publish_message(\n",
    "            #     publisher=publisher,\n",
    "            #     event_type=EventType.RETRIEVER,\n",
    "            #     user_sessions=user_sessions,\n",
    "            #     message=f\"SQL query execution output:{query_json}\",\n",
    "            #     event_data=query_json,\n",
    "            # )\n",
    "            if \"error\" in query_json and query_json[\"error\"] != \"\":\n",
    "                query = refiner(\n",
    "                    query=action,\n",
    "                    sql=query,\n",
    "                    error=query_response[\"error\"],\n",
    "                    exception_class=None,\n",
    "                )\n",
    "\n",
    "                continue\n",
    "            correct_flag = True\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(\"*********************************\")\n",
    "            exception_class = e.__class__.__name__\n",
    "            exception_message = e\n",
    "            print(e)\n",
    "            query = refiner(\n",
    "                query=action,\n",
    "                sql=query,\n",
    "                error=exception_message,\n",
    "                exception_class=exception_class,\n",
    "            )\n",
    "    if not correct_flag:\n",
    "        query_response = json.dumps({\"message\": \"An error occurred.\", \"data\": []})\n",
    "        query = \"\"\n",
    "    message = (\n",
    "        f\"Str: Action result for current ation:\\n{json.dumps(query_response, indent=2)}\"\n",
    "    )\n",
    "    print(message)\n",
    "\n",
    "    str_response_prompt = STR_RESPONSE_SYNTHESIZER.format(\n",
    "        question=action, sql=json_response[0][\"query\"], response=query_response\n",
    "    )\n",
    "\n",
    "    str_response = openai_completion(str_response_prompt)\n",
    "    str_response = json.loads(str_response)\n",
    "\n",
    "    query_json = json.loads(query_response)\n",
    "    df = pd.DataFrame(query_json[\"data\"])\n",
    "    file_url = \"\"\n",
    "    data = \"\"\n",
    "\n",
    "    num_rows = df.shape[0]\n",
    "    num_columns = df.shape[1]\n",
    "\n",
    "    if num_rows > 1 or num_columns > 2:\n",
    "\n",
    "        file_url = \"\"\n",
    "        df = df.loc[:, ~df.columns.duplicated()]\n",
    "        data = json.dumps(df.to_json(orient=\"records\", date_format=\"iso\"))\n",
    "\n",
    "    final_response = {\n",
    "        \"sql\": query,\n",
    "        \"csv\": file_url,\n",
    "        \"table\": data,\n",
    "        \"query_resultset\": query_response,\n",
    "        \"answer\": str_response[\"answer\"],\n",
    "        \"node_ids_used\": [node[\"id\"] for node in chunks_to_select],\n",
    "    }\n",
    "    return final_response\n",
    "\n",
    "\n",
    "def execute_unstructured_query(\n",
    "    action,\n",
    "    rows_content,\n",
    "    previous_action_execution_result,\n",
    "    unstr_prompt_template,\n",
    "    user_sessions,\n",
    "    run_id=None,\n",
    "):\n",
    "    concatenated_content, node_ids_used = concatenate_content(\n",
    "        rows_content, \"unstructured\"\n",
    "    )\n",
    "    message = f\"unstr node_ids_used:{node_ids_used}\"\n",
    "    print(message)\n",
    "\n",
    "\n",
    "    prompt = create_prompt(\n",
    "        action,\n",
    "        concatenated_content,\n",
    "        previous_action_execution_result,\n",
    "        unstr_prompt_template,\n",
    "    )\n",
    "    json_response = openai_completion(prompt)\n",
    "    # message = f\"Unstr Action result:\\n{json.dumps(json_response, indent=2)}\"\n",
    "    print(message)\n",
    "\n",
    "    json_response = json.loads(json_response)\n",
    "    print(\"+++++++++ json respopnse  +++++++++++\", json_response)\n",
    "    indexes = json_response[\"information_numbers\"]\n",
    "    node_ids_used = extract_elements(node_ids_used, indexes)\n",
    "    message = f\"node_ids_used to answer:{node_ids_used}\"\n",
    "    print(message)\n",
    "\n",
    "    final_response = {\n",
    "        \"sql\": \"\",\n",
    "        \"csv\": \"\",\n",
    "        \"table\": \"\",\n",
    "        \"query_resultset\": \"\",\n",
    "        \"answer\": json_response[\"answer\"],\n",
    "        \"node_ids_used\": node_ids_used,\n",
    "    }\n",
    "    return final_response\n",
    "\n",
    "\n",
    "def extract_elements(strings_list, indexes):\n",
    "    try:\n",
    "        # Ensure indexes are integers and convert 1-based indexes to 0-based\n",
    "        zero_based_indexes = [int(index) - 1 for index in indexes]\n",
    "\n",
    "        # Extract elements using the 0-based indexes\n",
    "        extracted_elements = [\n",
    "            strings_list[i] for i in zero_based_indexes if 0 <= i < len(strings_list)\n",
    "        ]\n",
    "        return extracted_elements\n",
    "\n",
    "    except Exception as e:\n",
    "        # Catch any other unexpected errors\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return []\n",
    "\n",
    "\n",
    "def execute_action_with_similarity(\n",
    "    action,\n",
    "    rows_content,\n",
    "    highest_unstructured_similarity,\n",
    "    highest_structured_similarity,\n",
    "    previous_action_execution_result,\n",
    "    str_prompt_template,\n",
    "    unstr_prompt_template,\n",
    "    user_sessions,\n",
    "):\n",
    "    if highest_structured_similarity > highest_unstructured_similarity:\n",
    "        message = \"Executing structured query (combined)\"\n",
    "        print(message)\n",
    "\n",
    "        return execute_structured_query(\n",
    "            action,\n",
    "            rows_content,\n",
    "            previous_action_execution_result,\n",
    "            str_prompt_template,\n",
    "            user_sessions=user_sessions,\n",
    "        )\n",
    "    elif highest_unstructured_similarity > highest_structured_similarity:\n",
    "        message = \"Executing unstructured query (combined)\"\n",
    "        print(message)\n",
    "\n",
    "        return execute_unstructured_query(\n",
    "            action,\n",
    "            rows_content,\n",
    "            previous_action_execution_result,\n",
    "            unstr_prompt_template,\n",
    "            user_sessions=user_sessions,\n",
    "        )\n",
    "\n",
    "\n",
    "def execute_action(\n",
    "    action,\n",
    "    rows_content,\n",
    "    unstructured_count,\n",
    "    structured_count,\n",
    "    previous_action_execution_result,\n",
    "    str_prompt_template,\n",
    "    unstr_prompt_template,\n",
    "    user_sessions,\n",
    "    run_id=None,\n",
    "):\n",
    "    if structured_count > unstructured_count:\n",
    "        message = \"Executing structured query (combined)\"\n",
    "        print()\n",
    "\n",
    "        return execute_structured_query(\n",
    "            action,\n",
    "            rows_content,\n",
    "            previous_action_execution_result,\n",
    "            str_prompt_template,\n",
    "            user_sessions=user_sessions,\n",
    "            run_id=run_id,\n",
    "        )\n",
    "    elif unstructured_count > structured_count:\n",
    "        message = \"Executing unstructured query (combined)\"\n",
    "        print(message)\n",
    "\n",
    "        return execute_unstructured_query(\n",
    "            action,\n",
    "            rows_content,\n",
    "            previous_action_execution_result,\n",
    "            unstr_prompt_template,\n",
    "            user_sessions=user_sessions,\n",
    "            run_id=run_id,\n",
    "        )\n",
    "\n",
    "\n",
    "def check_action_execution_result_quality(\n",
    "    action_execution_result, action, prompt_template\n",
    "):\n",
    "    # TODO: In production harmonize the output that comes out of structured and unstructired action execution. The following is an ugly hack for the ugly code that I wrote.\n",
    "\n",
    "    if (\n",
    "        action_execution_result is not None\n",
    "        and action_execution_result[\"query_resultset\"] != \"\"\n",
    "    ):\n",
    "        result_data = action_execution_result[\"query_resultset\"]\n",
    "    else:\n",
    "        result_data = action_execution_result[\"answer\"]\n",
    "\n",
    "    # TODO: Add the information (derived from questions - keywords, actions, informations) as well to the prompt to make it better\n",
    "    prompt = prompt_template.format(action=action, result_data=result_data)\n",
    "\n",
    "    response = (\n",
    "        client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,\n",
    "                }\n",
    "            ],\n",
    "            temperature=0,\n",
    "            model=OPENAI_MODEL,\n",
    "            max_tokens=4000,\n",
    "        )\n",
    "        .choices[0]\n",
    "        .message.content\n",
    "    )\n",
    "\n",
    "    json_response = extract_json_from_text(response)\n",
    "    print(f\"Action result:\\n{json.dumps(json_response, indent=2)}\")\n",
    "    return json_response\n",
    "\n",
    "\n",
    "def get_best_answer(question, answers, prompt_template=ANSWER_SELECTOR):\n",
    "    prompt = ANSWER_SELECTOR.format(question=question, responses=answers)\n",
    "    response = openai_completion(prompt=prompt)\n",
    "    response = json.loads(response)\n",
    "    return response.get(\"best_response\", \"Failed to select the best answer!!\")\n",
    "\n",
    "\n",
    "def get_final_answer(answer_dict):\n",
    "    simplified_keyword_question_emb = get_simplified_json(\n",
    "        answer_dict[\"keyword_question_emb\"]\n",
    "    )\n",
    "    simplified_keyword_chunk_emb = get_simplified_json(answer_dict[\"keyword_chunk_emb\"])\n",
    "    simplified_entity_chunk_emb = get_simplified_json(answer_dict[\"entity_chunk_emb\"])\n",
    "\n",
    "    final_answer_selector_prompt_str = final_answer_selector_prompt(\n",
    "        simplified_keyword_question_emb,\n",
    "        simplified_keyword_chunk_emb,\n",
    "        simplified_entity_chunk_emb,\n",
    "    )\n",
    "\n",
    "    print(\"Final Answer selector prompt: \",final_answer_selector_prompt_str)\n",
    "\n",
    "    simplified_keyword_question_emb_v2 = get_simplified_json_v2(\n",
    "        answer_dict[\"keyword_question_emb\"]\n",
    "    )\n",
    "    simplified_keyword_chunk_emb_v2 = get_simplified_json_v2(answer_dict[\"keyword_chunk_emb\"])\n",
    "    simplified_entity_chunk_emb_v2 = get_simplified_json_v2(answer_dict[\"entity_chunk_emb\"])\n",
    "\n",
    "    final_answer_selector_prompt_str_v2 = final_answer_selector_prompt(\n",
    "        simplified_keyword_question_emb_v2,\n",
    "        simplified_keyword_chunk_emb_v2,\n",
    "        simplified_entity_chunk_emb_v2,\n",
    "    )\n",
    "\n",
    "    print(\"Final Answer selector prompt V2: \",final_answer_selector_prompt_str_v2)\n",
    "\n",
    "    final_response = openai_completion_final_answer(prompt=final_answer_selector_prompt_str_v2)\n",
    "    final_response = json.loads(final_response)\n",
    "    print(\"++++++++ final response ++++++\", final_response)\n",
    "    return final_response, final_answer_selector_prompt_str, final_answer_selector_prompt_str_v2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action And Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Form Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=OPENAI_API_KEY,\n",
    ")\n",
    "\n",
    "def get_actions(\n",
    "    question,\n",
    "    question_analysis,\n",
    "    keywords,\n",
    "    actions,\n",
    "    retriever_fn=find_similar_embeddings_by_keyword_and_chunk_embedding,\n",
    "    role_name=[],\n",
    "    user_sessions=None,\n",
    "    run_id=None,\n",
    "):\n",
    "    roles = role_name\n",
    "\n",
    "    baseline = {}\n",
    "    baseline[\"question\"] = question\n",
    "    baseline[\"keywords\"] = question_analysis[0][\"keywords\"]\n",
    "    baseline[\"actions\"] = question_analysis[0][\"actions\"]\n",
    "    baseline[\"information\"] = question_analysis[0][\"information\"]\n",
    "    baseline[\"entities\"] = question_analysis[0][\"entities\"]\n",
    "    baseline[\"role\"] = roles\n",
    "    baseline[\"Similar_chunks_Question_AND\"] = []\n",
    "    baseline[\"Similar_chunks_Question_OR\"] = []\n",
    "    baseline[\"structured_count\"] = 0\n",
    "    baseline[\"unstructured_count\"] = 0\n",
    "    baseline[\"query_type\"] = \"\"\n",
    "\n",
    "    # Split each string into words and flatten the list\n",
    "    joined_keywords = get_joined_keywords(keywords=keywords)\n",
    "    message = f\"joined_keywords: {joined_keywords}\"\n",
    "    print(message)\n",
    "\n",
    "    # First lets decide if this is a structured only or unstructured only or combined query\n",
    "    # Ensure each action ends with a full stop and combine into a single paragraph\n",
    "    action_paragraph = \". \".join(action.rstrip(\".\") for action in actions) + \".\"\n",
    "    message = f\"Action paragraph: {action_paragraph}\"\n",
    "    print(message)\n",
    "\n",
    "    action_paragraph_response = client.embeddings.create(\n",
    "        input=action_paragraph, model=EMBEDDING_MODEL\n",
    "    )\n",
    "    action_paragraph_embedding = action_paragraph_response.data[0].embedding\n",
    "\n",
    "    # print(f\"Action paragraph embedding: {action_paragraph_embedding}\")\n",
    "    action_paragraph_embedding_str = (\n",
    "        \"[\" + \",\".join(map(str, action_paragraph_embedding)) + \"]\"\n",
    "    )\n",
    "    rows = retriever_fn(\n",
    "        action_paragraph_embedding_str, joined_words=joined_keywords, roles=roles\n",
    "    ) \n",
    "    unstructured_count = 0\n",
    "    structured_count = 0\n",
    "\n",
    "    if rows is not None and len(rows) > 0:\n",
    "        for row in rows:\n",
    "            row_dict = {\n",
    "                \"id\": row.id if row.id is not None else None,\n",
    "                \"chunk_content\": row.chunk_content\n",
    "                if row.chunk_content is not None\n",
    "                else None,\n",
    "                \"summary\": row.summary if row.summary is not None else None,\n",
    "                \"keywords_combined\": row.keywords_combined\n",
    "                if row.keywords_combined is not None\n",
    "                else None,\n",
    "                \"document_name\": row.document_name\n",
    "                if row.document_name is not None\n",
    "                else None,\n",
    "                \"document_url\": row.document_url\n",
    "                if row.document_url is not None\n",
    "                else None,\n",
    "                \"table_name\": row.table_name if row.table_name is not None else None,\n",
    "                \"database_name\": row.database_name\n",
    "                if row.database_name is not None\n",
    "                else None,\n",
    "                \"page_number\": row.page_number if row.page_number is not None else None,\n",
    "                \"similarity\": row.similarity if row.similarity is not None else None,\n",
    "            }\n",
    "            baseline[\"Similar_chunks_Question_AND\"].append(row)\n",
    "            row_json = json.dumps(row_dict, indent=4)\n",
    "            print(\n",
    "                \"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\"\n",
    "            )\n",
    "            print(row_json)\n",
    "            print(\n",
    "                \"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\"\n",
    "            )\n",
    "            # Adding the type attribute based on conditions\n",
    "            if not row_dict[\"document_name\"] and row_dict[\"table_name\"]:\n",
    "                row_dict[\"type\"] = \"structured\"\n",
    "                structured_count += 1\n",
    "            elif not row_dict[\"table_name\"] and row_dict[\"document_name\"]:\n",
    "                row_dict[\"type\"] = \"unstructured\"\n",
    "                unstructured_count += 1\n",
    "    else:\n",
    "        print(\n",
    "            \"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\"\n",
    "        )\n",
    "        print(\"No similar embeddings found.\")\n",
    "        print(\n",
    "            \"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\"\n",
    "        )\n",
    "\n",
    "        # Split each string into words and flatten the list\n",
    "        # flattened_terms = [word for term in keywords for word in term.split()]\n",
    "        joined_keywords = get_joined_keywords(\n",
    "            keywords=keywords, seperator=\"|\"\n",
    "        )  # '|'.join(flattened_terms)\n",
    "        print(f\"joined_keywords: {joined_keywords}\")\n",
    "        rows = retriever_fn(\n",
    "            action_paragraph_embedding_str, joined_words=joined_keywords, roles=roles\n",
    "        )\n",
    "\n",
    "        unstructured_count = 0\n",
    "        structured_count = 0\n",
    "        for row in rows:\n",
    "            row_dict = {\n",
    "                \"id\": row.id if row.id is not None else None,\n",
    "                \"chunk_content\": row.chunk_content\n",
    "                if row.chunk_content is not None\n",
    "                else None,\n",
    "                \"summary\": row.summary if row.summary is not None else None,\n",
    "                \"keywords_combined\": row.keywords_combined\n",
    "                if row.keywords_combined is not None\n",
    "                else None,\n",
    "                \"document_name\": row.document_name\n",
    "                if row.document_name is not None\n",
    "                else None,\n",
    "                \"document_url\": row.document_url\n",
    "                if row.document_url is not None\n",
    "                else None,\n",
    "                \"table_name\": row.table_name if row.table_name is not None else None,\n",
    "                \"database_name\": row.database_name\n",
    "                if row.database_name is not None\n",
    "                else None,\n",
    "                \"page_number\": row.page_number if row.page_number is not None else None,\n",
    "                \"similarity\": row.similarity if row.similarity is not None else None,\n",
    "            }\n",
    "            baseline[\"Similar_chunks_Question_OR\"].append(row)\n",
    "            row_json = json.dumps(row_dict, indent=4)\n",
    "            print(\n",
    "                \"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\"\n",
    "            )\n",
    "            print(row_json)\n",
    "            print(\n",
    "                \"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\"\n",
    "            )\n",
    "            # Adding the type attribute based on conditions\n",
    "            if not row_dict[\"document_name\"] and row_dict[\"table_name\"]:\n",
    "                row_dict[\"type\"] = \"structured\"\n",
    "                structured_count += 1\n",
    "            elif not row_dict[\"table_name\"] and row_dict[\"document_name\"]:\n",
    "                row_dict[\"type\"] = \"unstructured\"\n",
    "                unstructured_count += 1\n",
    "\n",
    "    baseline[\"structured_count\"] = structured_count\n",
    "    baseline[\"unstructured_count\"] = unstructured_count\n",
    "\n",
    "    query_type = \"\"\n",
    "    if structured_count == 0 and unstructured_count > 0:\n",
    "        query_type = \"unstructured\"\n",
    "        message = \"Executing unstructured query (only)\"\n",
    "        print(message)\n",
    "\n",
    "        actions = [action_paragraph]\n",
    "    elif unstructured_count == 0 and structured_count > 0:\n",
    "        query_type = \"structured\"\n",
    "        message = \"Executing structured query (only)\"\n",
    "        print(message)\n",
    "        actions = [action_paragraph]\n",
    "    else:\n",
    "        query_type = \"combined\"\n",
    "        message = \"Executing combined query\"\n",
    "        print(message)\n",
    "\n",
    "    baseline[\"query_type\"] = query_type\n",
    "\n",
    "    baseline_list = []\n",
    "    baseline_list.append(baseline[\"question\"])\n",
    "    baseline_list.append(baseline[\"keywords\"])\n",
    "    baseline_list.append(baseline[\"actions\"])\n",
    "    baseline_list.append(baseline[\"information\"])\n",
    "    baseline_list.append(baseline[\"entities\"])\n",
    "    baseline_list.append(baseline[\"role\"])\n",
    "    baseline_list.append(baseline[\"Similar_chunks_Question_AND\"])\n",
    "    baseline_list.append(baseline[\"Similar_chunks_Question_OR\"])\n",
    "    baseline_list.append(baseline[\"structured_count\"])\n",
    "    baseline_list.append(baseline[\"unstructured_count\"])\n",
    "    baseline_list.append(baseline[\"query_type\"])\n",
    "\n",
    "    return actions, baseline_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execute Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=OPENAI_API_KEY,\n",
    ")\n",
    "\n",
    "\n",
    "def get_action_response(\n",
    "    question,\n",
    "    actions,\n",
    "    keywords,\n",
    "    types,\n",
    "    baseline_list,\n",
    "    retriever_fn=find_similar_embeddings_by_keyword_and_chunk_embedding,\n",
    "    role_name=[],\n",
    "    user_sessions=None,\n",
    "    run_id=None,\n",
    "):\n",
    "    print(\"++++++++++++ actions +++++++++++++\",actions)\n",
    "    previous_action_execution_result = \"\"\n",
    "\n",
    "    roles = role_name\n",
    "\n",
    "    final_baseline_list = []\n",
    "\n",
    "    ## Response\n",
    "    if retriever_fn == find_similar_embeddings_by_keyword_and_chunk_embedding:\n",
    "        retriever_flow = \"keyword\"\n",
    "    elif retriever_fn == find_similar_embeddings_by_entities_and_chunk_embedding:\n",
    "        retriever_flow = \"entity\"\n",
    "    elif retriever_fn == find_similar_embeddings_by_keyword_and_questions_embedding:\n",
    "        retriever_flow = \"question\"\n",
    "\n",
    "    actions_list = {\n",
    "        \"question\": question,\n",
    "        \"retriever_type\": retriever_flow,\n",
    "        \"keywords\": keywords\n",
    "        if (retriever_flow == \"keyword\" or retriever_flow == \"question\")\n",
    "        else [],\n",
    "        \"entities\": [] if retriever_flow == \"entity\" else [],\n",
    "        \"actions\": {},\n",
    "    }\n",
    "    action_no = 1\n",
    "\n",
    "    final_response = {\"answer\": \"\", \"chunk_nodes\": [], \"document_nodes\": []}\n",
    "    action_execution_result = {\n",
    "        \"query_resultset\": \"\",\n",
    "        \"answer\": \"No Answer Found\",\n",
    "    }\n",
    "\n",
    "\n",
    "    for i in range(len(actions)):\n",
    "        action=actions[i]\n",
    "        keyword=keywords[i] if keywords!=[] else \"\"\n",
    "        type=types[i]\n",
    "\n",
    "        print(action,keyword,type)\n",
    "\n",
    "        flattened_terms = [word for term in keyword for word in term.split()]\n",
    "        joined_keywords = '|'.join(flattened_terms)\n",
    "        print(f\"joined_keywords: {joined_keywords}\")\n",
    "\n",
    "        ## Response\n",
    "        current_action = {\n",
    "            \"current_action\": \"\",\n",
    "            \"action_type\": \"\",\n",
    "            \"sql\": \"\",\n",
    "            \"table\": \"\",\n",
    "            \"csv\": \"\",\n",
    "            \"answer\": \"\",\n",
    "            \"document_level\": {},\n",
    "            \"chunk_level\": {},\n",
    "        }\n",
    "\n",
    "        baseline_action = {}\n",
    "        baseline_action[\"Current_Action\"] = \"\"\n",
    "        baseline_action[\"Action_chunks\"] = []\n",
    "        baseline_action[\"Action_Similarity_Score\"] = []\n",
    "        baseline_action[\"action_structured_count\"] = 0\n",
    "        baseline_action[\"action_unstructured_count\"] = 0\n",
    "        baseline_action[\"action_highest_structured_similarity\"] = \"\"\n",
    "        baseline_action[\"action_highest_unstructured_similarity\"] = \"\"\n",
    "        baseline_action[\"Action_query_sql\"] = \"\"\n",
    "        baseline_action[\"Action_query_resultset\"] = \"\"\n",
    "        baseline_action[\"Action_answer\"] = \"\"\n",
    "        baseline_action[\"Action_Quality\"] = \"\"\n",
    "        baseline_action[\"Action_query_sql_with_similarity\"] = \"\"\n",
    "        baseline_action[\"Action_query_resultset_with_similarity\"] = \"\"\n",
    "        baseline_action[\"Action_answer_with_similarity\"] = \"\"\n",
    "        baseline_action[\"final_answer\"] = \"\"\n",
    "\n",
    "        temp_list = deepcopy(baseline_list)\n",
    "        message = f\"Current Action: {action}\"\n",
    "        print(message)\n",
    "        # baseline_action = copy.deepcopy(baseline)\n",
    "        if (\n",
    "            previous_action_execution_result is not None\n",
    "            and len(previous_action_execution_result) > 0\n",
    "        ):\n",
    "            print(\n",
    "                f\"Previous action execution result: {previous_action_execution_result}\"\n",
    "            )\n",
    "            prompt = f\"\"\"\n",
    "\n",
    "            You are a helpful assitant and your task is to execute a given action.\n",
    "            Atlest one action has been executed and here is the outcome of the previous action execution(s).\n",
    "\n",
    "            Previous action outcome(s): {previous_action_execution_result}\n",
    "\n",
    "            The next action to be performed is: {action}\n",
    "\n",
    "            Based on the information provided in the previous action outcome, modify the current action only and only if it enriches it further.\n",
    "            You can add information from the previous action outcome to the current action.\n",
    "            However, under no circumstance should the context / intent / meaning of the current action be changed.\n",
    "            It is also fine to just leave the current action as is if no modification is required - Just return the current action as is.\n",
    "            If the next action has an identifier and the previous action outcome has details of the identifier, you must include all the details in the current action.\n",
    "\n",
    "            Use below template for output:\n",
    "\n",
    "            ```json\n",
    "            {{\n",
    "                \"new_action\": \"\"\n",
    "            }}\n",
    "            ```\n",
    "            \"\"\"\n",
    "\n",
    "            print(\n",
    "                \"=====================================================================\"\n",
    "            )\n",
    "            print(f\"Prompt for action modification: {prompt}\")\n",
    "            print(\n",
    "                \"=====================================================================\"\n",
    "            )\n",
    "\n",
    "            response = (\n",
    "                client.chat.completions.create(\n",
    "                    messages=[\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": prompt,\n",
    "                        }\n",
    "                    ],\n",
    "                    temperature=0,\n",
    "                    model=OPENAI_MODEL_4O,\n",
    "                    max_tokens=4000,\n",
    "                )\n",
    "                .choices[0]\n",
    "                .message.content\n",
    "            )\n",
    "\n",
    "            json_response = extract_json_from_text(response)\n",
    "            print(f\"Action modification result:\\n{json.dumps(json_response, indent=2)}\")\n",
    "            if len(json_response) > 0:\n",
    "                new_action = json_response[0][\"new_action\"]\n",
    "                action = new_action\n",
    "\n",
    "        baseline_action[\"Current_Action\"] = action\n",
    "        current_action[\"current_action\"] = action\n",
    "\n",
    "        action_response = client.embeddings.create(input=action, model=EMBEDDING_MODEL)\n",
    "        action_embedding = action_response.data[0].embedding\n",
    "\n",
    "        # print(f\"Action embedding: {action_embedding}\")\n",
    "        action_embedding_str = \"[\" + \",\".join(map(str, action_embedding)) + \"]\"\n",
    "        rows = retriever_fn(\n",
    "            action_embedding_str, joined_words=joined_keywords, roles=roles, source_type=type\n",
    "        )\n",
    "\n",
    "        structured_node_ids = []\n",
    "        unstructured_node_ids = []\n",
    "        response_node_ids = []\n",
    "\n",
    "        print(\"+++++++++++++++++++++++++++++  rows+++++\", rows)\n",
    "\n",
    "        if rows is not None and len(rows) > 0:\n",
    "            all_rows_content = []\n",
    "            unstructured_count = 0\n",
    "            structured_count = 0\n",
    "            highest_unstructured_similarity = 0\n",
    "            highest_structured_similarity = 0\n",
    "\n",
    "            for row in rows:\n",
    "                row_dict = {\n",
    "                    \"id\": row.id if row.id is not None else None,\n",
    "                    \"source_id\": row.source_id if row.source_id is not None else None,\n",
    "                    \"chunk_content\": row.chunk_content\n",
    "                    if row.chunk_content is not None\n",
    "                    else None,\n",
    "                    \"summary\": row.summary if row.summary is not None else None,\n",
    "                    \"keywords\": row.keywords if row.keywords is not None else None,\n",
    "                    \"entities\": row.entities if row.entities is not None else None,\n",
    "                    \"questions\": row.questions if row.questions is not None else None,\n",
    "                    \"keywords_combined\": row.keywords_combined\n",
    "                    if row.keywords_combined is not None\n",
    "                    else None,\n",
    "                    \"document_name\": row.document_name\n",
    "                    if row.document_name is not None\n",
    "                    else None,\n",
    "                    \"document_url\": row.document_url\n",
    "                    if row.document_url is not None\n",
    "                    else None,\n",
    "                    \"table_name\": row.table_name\n",
    "                    if row.table_name is not None\n",
    "                    else None,\n",
    "                    \"database_name\": row.database_name\n",
    "                    if row.database_name is not None\n",
    "                    else None,\n",
    "                    \"page_number\": row.page_number\n",
    "                    if row.page_number is not None\n",
    "                    else None,\n",
    "                    \"similarity\": row.similarity\n",
    "                    if row.similarity is not None\n",
    "                    else None,\n",
    "                }\n",
    "                baseline_action[\"Action_chunks\"].append(row_dict)\n",
    "                baseline_action[\"Action_Similarity_Score\"].append(\n",
    "                    row_dict[\"similarity\"]\n",
    "                )\n",
    "\n",
    "                # Adding the type attribute based on conditions\n",
    "                if not row_dict[\"document_name\"] and row_dict[\"table_name\"]:\n",
    "                    row_dict[\"type\"] = \"structured\"\n",
    "                    structured_node_ids.append(str(row_dict[\"id\"]))\n",
    "                    structured_count += 1\n",
    "                    if highest_structured_similarity < row_dict[\"similarity\"]:\n",
    "                        highest_structured_similarity = row_dict[\"similarity\"]\n",
    "                elif not row_dict[\"table_name\"] and row_dict[\"document_name\"]:\n",
    "                    row_dict[\"type\"] = \"unstructured\"\n",
    "                    unstructured_node_ids.append(str(row_dict[\"id\"]))\n",
    "                    unstructured_count += 1\n",
    "                    if highest_unstructured_similarity < row_dict[\"similarity\"]:\n",
    "                        highest_unstructured_similarity = row_dict[\"similarity\"]\n",
    "                row_json = json.dumps(row_dict, indent=4)\n",
    "                all_rows_content.append(row_dict)\n",
    "                print(\n",
    "                    \"-----------------------------------------------------------------------------------------------------------------\"\n",
    "                )\n",
    "                print(row_json)\n",
    "                print(\n",
    "                    \"-----------------------------------------------------------------------------------------------------------------\"\n",
    "                )\n",
    "\n",
    "            baseline_action[\"action_structured_count\"] = structured_count\n",
    "            baseline_action[\"action_unstructured_count\"] = unstructured_count\n",
    "            baseline_action[\"action_highest_structured_similarity\"] = (\n",
    "                highest_structured_similarity\n",
    "            )\n",
    "            baseline_action[\"action_highest_unstructured_similarity\"] = (\n",
    "                highest_unstructured_similarity\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                \"-----------------------------------------------------------------------------------------------------------------\"\n",
    "            )\n",
    "            print(f\"highest_unstructured_similarity: {highest_unstructured_similarity}\")\n",
    "            print(f\"highest_structured_similarity: {highest_structured_similarity}\")\n",
    "            print(\n",
    "                \"-----------------------------------------------------------------------------------------------------------------\"\n",
    "            )\n",
    "\n",
    "            # Now take the content of all_rows_content and try to execute the first action\n",
    "            action_execution_result = execute_action(\n",
    "                action,\n",
    "                all_rows_content,\n",
    "                unstructured_count,\n",
    "                structured_count,\n",
    "                previous_action_execution_result,\n",
    "                STR_ACTION_EXECUTOR,\n",
    "                UNSTR_ACTION_EXECUTOR,\n",
    "                user_sessions=user_sessions,\n",
    "                run_id=run_id,\n",
    "            )\n",
    "            baseline_action[\"Action_query_sql\"] = (\n",
    "                action_execution_result[\"sql\"] if action_execution_result else \"\"\n",
    "            )\n",
    "            baseline_action[\"Action_query_resultset\"] = (\n",
    "                action_execution_result[\"query_resultset\"]\n",
    "                if action_execution_result\n",
    "                else \"\"\n",
    "            )\n",
    "            baseline_action[\"Action_answer\"] = (\n",
    "                action_execution_result[\"answer\"] if action_execution_result else \"\"\n",
    "            )\n",
    "            message = f\"Action execution result:\\n{action_execution_result}\"\n",
    "            print(message)\n",
    "\n",
    "            # verify the action execution result agains the given action\n",
    "            action_execution_result_quality = check_action_execution_result_quality(\n",
    "                action_execution_result, action, prompt_template=QUALITY_CHECKER\n",
    "            )\n",
    "            # action_execution_result_quality[0][\"quality\"] = 'Low' #TODO: Delete this line\n",
    "            baseline_action[\"Action_Quality\"] = action_execution_result_quality[0][\n",
    "                \"quality\"\n",
    "            ]\n",
    "            message = (\n",
    "                f\"Action execution result quality: {action_execution_result_quality}\"\n",
    "            )\n",
    "            print(message)\n",
    "\n",
    "\n",
    "            if structured_count > unstructured_count:\n",
    "                current_action[\"action_type\"] = \"Structured\"\n",
    "            else:\n",
    "                current_action[\"action_type\"] = \"Unstructured\"\n",
    "\n",
    "            response_node_ids = (\n",
    "                action_execution_result[\"node_ids_used\"]\n",
    "                if action_execution_result\n",
    "                else []\n",
    "            )\n",
    "\n",
    "            current_action[\"sql\"] = (\n",
    "                action_execution_result[\"sql\"] if action_execution_result else \"\"\n",
    "            )\n",
    "            current_action[\"table\"] = (\n",
    "                action_execution_result[\"table\"] if action_execution_result else \"\"\n",
    "            )\n",
    "            current_action[\"csv\"] = (\n",
    "                action_execution_result[\"csv\"] if action_execution_result else \"\"\n",
    "            )\n",
    "            current_action[\"document_level\"] = get_document_level_dict(\n",
    "                response_node_ids, all_rows_content\n",
    "            )\n",
    "            current_action[\"chunk_level\"] = get_chunk_level_dict(\n",
    "                response_node_ids, all_rows_content, flattened_terms, retriever_flow\n",
    "            )\n",
    "\n",
    "            if action_execution_result_quality[0][\"quality\"] == \"Low\":\n",
    "                message = \"Action execution result quality is low. Using similarity score to perform action.\"\n",
    "                print(message)\n",
    "\n",
    "                action_execution_result = execute_action_with_similarity(\n",
    "                    action,\n",
    "                    all_rows_content,\n",
    "                    highest_unstructured_similarity,\n",
    "                    highest_structured_similarity,\n",
    "                    previous_action_execution_result,\n",
    "                    STR_ACTION_EXECUTOR,\n",
    "                    UNSTR_ACTION_EXECUTOR,\n",
    "                    user_sessions=user_sessions,\n",
    "                )\n",
    "\n",
    "                baseline_action[\"Action_query_sql_with_similarity\"] = (\n",
    "                    action_execution_result[\"sql\"] if action_execution_result else \"\"\n",
    "                )\n",
    "                baseline_action[\"Action_query_resultset_with_similarity\"] = (\n",
    "                    action_execution_result[\"query_resultset\"]\n",
    "                    if action_execution_result\n",
    "                    else \"\"\n",
    "                )\n",
    "                baseline_action[\"Action_answer_with_similarity\"] = (\n",
    "                    action_execution_result[\"answer\"] if action_execution_result else \"\"\n",
    "                )\n",
    "\n",
    "                if highest_structured_similarity > highest_unstructured_similarity:\n",
    "                    current_action[\"action_type\"] = \"Structured\"\n",
    "                else:\n",
    "                    current_action[\"action_type\"] = \"Unstructured\"\n",
    "\n",
    "                response_node_ids = (\n",
    "                    action_execution_result[\"node_ids_used\"]\n",
    "                    if action_execution_result\n",
    "                    else []\n",
    "                )\n",
    "\n",
    "                current_action[\"sql\"] = (\n",
    "                    action_execution_result[\"sql\"] if action_execution_result else \"\"\n",
    "                )\n",
    "                current_action[\"table\"] = (\n",
    "                    action_execution_result[\"table\"] if action_execution_result else \"\"\n",
    "                )\n",
    "                current_action[\"csv\"] = (\n",
    "                    action_execution_result[\"csv\"] if action_execution_result else \"\"\n",
    "                )\n",
    "                current_action[\"document_level\"] = get_document_level_dict(\n",
    "                    response_node_ids, all_rows_content\n",
    "                )\n",
    "                current_action[\"chunk_level\"] = get_chunk_level_dict(\n",
    "                    response_node_ids, all_rows_content, flattened_terms, retriever_flow\n",
    "                )\n",
    "\n",
    "            baseline_action[\"final_answer\"] = (\n",
    "                action_execution_result[\"answer\"] if action_execution_result else \"\"\n",
    "            )\n",
    "            current_action[\"answer\"] = (\n",
    "                action_execution_result[\"answer\"]\n",
    "                if action_execution_result\n",
    "                else \"No Answer Found\"\n",
    "            )\n",
    "            previous_action_execution_result = (\n",
    "                action_execution_result[\"answer\"] if action_execution_result else \"\"\n",
    "            )\n",
    "            message = f\"Completed Current Action : {action}\"\n",
    "        else:\n",
    "            message = f\"No similar embeddings found for the current action : {action}.\"\n",
    "            print(message)\n",
    "\n",
    "            action_execution_result = {\n",
    "                \"query_resultset\": \"\",\n",
    "                \"answer\": \"No Answer Found\",\n",
    "            }\n",
    "            baseline_action[\"final_answer\"] = action_execution_result[\"answer\"]\n",
    "\n",
    "        final_response[\"chunk_nodes\"] = list(\n",
    "            set(final_response[\"chunk_nodes\"] + response_node_ids)\n",
    "        )\n",
    "\n",
    "        ## added  data for each action\n",
    "        temp_list.append(baseline_action[\"Current_Action\"])\n",
    "        temp_list.append(baseline_action[\"Action_chunks\"])\n",
    "        temp_list.append(baseline_action[\"Action_Similarity_Score\"])\n",
    "        temp_list.append(baseline_action[\"action_structured_count\"])\n",
    "        temp_list.append(baseline_action[\"action_unstructured_count\"])\n",
    "        temp_list.append(baseline_action[\"action_highest_structured_similarity\"])\n",
    "        temp_list.append(baseline_action[\"action_highest_unstructured_similarity\"])\n",
    "        temp_list.append(baseline_action[\"Action_query_sql\"])\n",
    "        temp_list.append(baseline_action[\"Action_query_resultset\"])\n",
    "        temp_list.append(baseline_action[\"Action_answer\"])\n",
    "        temp_list.append(baseline_action[\"Action_Quality\"])\n",
    "        temp_list.append(baseline_action[\"Action_query_sql_with_similarity\"])\n",
    "        temp_list.append(baseline_action[\"Action_query_resultset_with_similarity\"])\n",
    "        temp_list.append(baseline_action[\"Action_answer_with_similarity\"])\n",
    "        temp_list.append(baseline_action[\"final_answer\"])\n",
    "\n",
    "        final_baseline_list.append(temp_list)\n",
    "\n",
    "        ## Response\n",
    "\n",
    "        current_action_dict = {f\"action_{action_no}\": current_action}\n",
    "        action_no += 1\n",
    "\n",
    "        actions_list[\"actions\"].update(current_action_dict)\n",
    "\n",
    "    final_response[\"answer\"] = (\n",
    "        action_execution_result[\"answer\"]\n",
    "        if action_execution_result\n",
    "        else \"No Answer Found\"\n",
    "    )\n",
    "    # final_response[\"node_ids_used\"] = action_execution_result[\"node_ids_used\"]\n",
    "\n",
    "    print(\"+++++++ action_list ++++++++\", actions_list)\n",
    "\n",
    "    return final_response, final_baseline_list, actions_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old Functions from Chat History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_openai_embeddings(text: str, model: str = EMBEDDING_MODEL):\n",
    "    \"\"\"\n",
    "    Retrieves the embedding for a given text using the specified model.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to embed.\n",
    "        model (str): The embedding model to use.\n",
    "\n",
    "    Returns:\n",
    "        list: The embedding vector for the given text.\n",
    "    \"\"\"\n",
    "    if text=='': \n",
    "        return None\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return json.dumps(client.embeddings.create(input=[text], model=model).data[0].embedding)\n",
    "\n",
    "def generate_question_embeddings(text: str, model: str = EMBEDDING_MODEL):\n",
    "    \"\"\"\n",
    "    Retrieves the embedding for a given text using the specified model.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to embed.\n",
    "        model (str): The embedding model to use.\n",
    "\n",
    "    Returns:\n",
    "        list: The embedding vector for the given text.\n",
    "    \"\"\"\n",
    "    if text == '':\n",
    "        return None\n",
    "\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    # Return the embedding as a list of floats, not a JSON string\n",
    "    return client.embeddings.create(input=[text], model=model).data[0].embedding\n",
    "\n",
    "\n",
    "def openai_completion_question_selector(prompt):\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=OPENAI_MODEL,\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant designed to output JSON.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "    # Extract the content from the response\n",
    "    result_content = response.choices[0].message.content\n",
    "    \n",
    "    try:\n",
    "        # Attempt to parse the response as JSON\n",
    "        result_json = json.loads(result_content)\n",
    "        return result_json\n",
    "    except json.JSONDecodeError:\n",
    "        # Handle the case where the response isn't valid JSON\n",
    "        print(\"Error: Response is not valid JSON.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_for_question_match(original_question, rows):\n",
    "    \"\"\"\n",
    "    This function creates a prompt for querying the OpenAI API to match the original question\n",
    "    with the most relevant question from the provided rows.\n",
    "\n",
    "    Args:\n",
    "    original_question (str): The question asked by the user.\n",
    "    rows (list): A list of dictionaries containing run_id, question, and other metadata.\n",
    "    \n",
    "    Returns:\n",
    "    str: The generated prompt string.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create the table from the rows\n",
    "    table = \"\\n\".join([f\"| {idx+1} | {row['run_id']} | {row['question']} |\" for idx, row in enumerate(rows)])\n",
    "\n",
    "    # Create the final prompt with instructions and the question table\n",
    "    prompt = f\"\"\"\n",
    "    You are tasked with selecting the most relevant question from a list of previously asked questions, based on a new question. \n",
    "    The aim is to find a question that either results in the **same outcome** or has the **same semantic meaning** as the new question. \n",
    "    Your role is to return the serial number of the most relevant question.\n",
    "\n",
    "    ### Provided Information:\n",
    "    1. **Original Question Asked**: \"{original_question}\"\n",
    "    2. **Table of Top Matched Questions**:\n",
    "    | Serial Number | Run ID  | Question                    |\n",
    "    | ------------- | ------- | --------------------------- |\n",
    "    {table}\n",
    "\n",
    "    ### Instructions:\n",
    "    - Select the **most relevant** question from the table.\n",
    "    - Return the **serial number** of the selected question in the following JSON format:\n",
    "      ```\n",
    "      {{\"question_no\": SERIAL_NUMBER, \"reasoning\":}}\n",
    "      ```\n",
    "    - If none of the questions are relevant, respond with:\n",
    "      ```\n",
    "      {{\"question_no\": 50, \"reasoning\":}}\n",
    "      ```\n",
    "    \"\"\"\n",
    "\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KPI Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Database Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "\n",
    "def get_top_matched_glossary(user_id, question_embedding, top_n=5):\n",
    "    \"\"\"\n",
    "    Retrieve top N matched glossary entries based on question embedding similarity using Python.\n",
    "\n",
    "    Args:\n",
    "        user_id (str): User ID for filtering glossary entries.\n",
    "        question_embedding (list): Embedding of the question as a list of floats.\n",
    "        top_n (int): Number of top matched entries to retrieve (default is 5).\n",
    "\n",
    "    Returns:\n",
    "        List[dict]: A list of dictionaries containing matched glossary entries with keys:\n",
    "                    'term_name', 'term_description', 'metric_sql_query'.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Create SQLAlchemy engine and session\n",
    "    engine = create_engine(DATABASE_URL_GLOSSARY_TABLE)\n",
    "    Session = sessionmaker(bind=engine)\n",
    "    session = Session()\n",
    "    try:\n",
    "        # SQL query to fetch embeddings and related information\n",
    "        query = text(\"\"\"\n",
    "        SELECT\n",
    "            term_name,\n",
    "            term_description,\n",
    "            metric_sql_query,\n",
    "            embedding_of_name,\n",
    "            embedding_of_description\n",
    "        FROM\n",
    "            glossary\n",
    "        WHERE\n",
    "            user_id = :user_id\n",
    "        \"\"\")\n",
    "\n",
    "        # Execute the query\n",
    "        results = session.execute(query, {\"user_id\": user_id}).fetchall()\n",
    "\n",
    "        matched_entries = []\n",
    "\n",
    "        # Calculate cosine similarity for each entry\n",
    "        for row in results:\n",
    "            term_name = row.term_name\n",
    "            term_description = row.term_description\n",
    "            metric_sql_query = row.metric_sql_query\n",
    "            embedding_of_name = row.embedding_of_name\n",
    "            embedding_of_description = row.embedding_of_description\n",
    "\n",
    "            # Calculate cosine similarity for both name and description embeddings\n",
    "            similarity_name = 1 - cosine(question_embedding, embedding_of_name) if embedding_of_name else 0\n",
    "            similarity_description = 1 - cosine(question_embedding, embedding_of_description) if embedding_of_description else 0\n",
    "\n",
    "            # Combine similarities (you can adjust the weight as needed)\n",
    "            combined_similarity = similarity_name + similarity_description\n",
    "\n",
    "            # Append the result with the combined similarity score\n",
    "            matched_entries.append({\n",
    "                \"term_name\": term_name,\n",
    "                \"term_description\": term_description,\n",
    "                \"metric_sql_query\": metric_sql_query,\n",
    "                \"similarity\": combined_similarity\n",
    "            })\n",
    "\n",
    "        # Sort entries by similarity and return the top N matches\n",
    "        matched_entries.sort(key=lambda x: x[\"similarity\"], reverse=True)\n",
    "        return matched_entries[:top_n]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching matched glossary entries: {e}\")\n",
    "        return []\n",
    "\n",
    "    finally:\n",
    "        # Close the session\n",
    "        session.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OpenAI Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_modified_question_with_glossary(question, top_matches):\n",
    "    \"\"\"\n",
    "    Generates a prompt for OpenAI to determine if the question contains any terms from the glossary data.\n",
    "    If it does, modifies the question by adding relevant glossary definitions and provides reasoning.\n",
    "\n",
    "    Args:\n",
    "        question (str): The original user question.\n",
    "        top_matches (list): Top 5 matched glossary entries, each containing 'term_name', 'term_description', and 'metric_sql_query'.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing 'modified_question' and 'reasoning'.\n",
    "    \"\"\"\n",
    "    # Prepare glossary data for the prompt\n",
    "    glossary_data_text = \"\\n\".join(\n",
    "        [f\"{i + 1}. {match['term_name']}: {match['term_description']}\" for i, match in enumerate(top_matches)]\n",
    "    )\n",
    "\n",
    "    # Construct the prompt with escaped curly braces\n",
    "    prompt = f\"\"\"\n",
    "You are a helpful assistant. I will provide you with a user question and a list of glossary terms with their descriptions.\n",
    "Your task is to:\n",
    "1. Determine if the user question includes any terms from the provided glossary data.\n",
    "2. If relevant terms are found, modify the question by incorporating the glossary definitions to provide additional context and clarity.\n",
    "3. If no modification is needed, return the question as it is.\n",
    "\n",
    "### Input:\n",
    "- User Question: \\\"{question}\\\"\n",
    "- Glossary Data:\n",
    "{glossary_data_text}\n",
    "\n",
    "### Output Format:\n",
    "Return a JSON response with the following fields:\n",
    "- \"modified_question\" (string): The modified question if terms are found; otherwise, return the original question.\n",
    "- \"reasoning\" (string): A brief explanation of why the question was modified or why no modification was needed.\n",
    "\n",
    "Example Output:\n",
    "{{\n",
    "    \"modified_question\": \"What is the impact of shrinkage (reduction in size or quantity over time) on the company's revenue?\",\n",
    "    \"reasoning\": \"The term 'shrinkage' was identified in the glossary and its definition was added for clarity.\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "    # Call OpenAI API using the existing function\n",
    "    response = get_keywords_linkedq_typeinfo_from_question(question, prompt)\n",
    "\n",
    "    # Debugging: Print the response to inspect its format\n",
    "    # print(\"API Response:\", response)\n",
    "\n",
    "    # Check if the response is a list or a dictionary\n",
    "    if isinstance(response, list):\n",
    "        # Assume the first item in the list is the dictionary we need\n",
    "        response = response[0]\n",
    "\n",
    "    # Extract the modified question and reasoning from the response\n",
    "    modified_question = response.get(\"modified_question\", question)\n",
    "    reasoning = response.get(\"reasoning\", \"No modification was needed.\")\n",
    "\n",
    "    return {\"modified_question\": modified_question, \"reasoning\": reasoning}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concurrent Call For Retrievers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modify type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_type_for_each_action(\n",
    "    question,\n",
    "    question_analysis,\n",
    "    retriever_fn,\n",
    "    role_name,\n",
    "):\n",
    "    \n",
    "    actions, keywords, informations, types = extract_analysis_details(question_analysis)\n",
    "    roles=role_name\n",
    "\n",
    "    for i in range(len(actions)):\n",
    "        action=actions[i]\n",
    "        keyword=keywords[i]\n",
    "\n",
    "        # Split each string into words and flatten the list\n",
    "        joined_keywords = get_joined_keywords(keyword)\n",
    "        print(f\"joined_keywords: {joined_keywords}\")\n",
    "\n",
    "        action_response = client.embeddings.create(input=action, model=EMBEDDING_MODEL)\n",
    "        action_embedding = action_response.data[0].embedding\n",
    "\n",
    "        # print(f\"Action paragraph embedding: {action_paragraph_embedding}\")\n",
    "        action_paragraph_embedding_str = '[' + ','.join(map(str, action_embedding)) + ']'\n",
    "\n",
    "        rows = retriever_fn(action_paragraph_embedding_str, joined_words=joined_keywords, roles=roles)\n",
    "        unstructured_count = 0\n",
    "        structured_count = 0\n",
    "\n",
    "        if rows is not None and len(rows) > 0:\n",
    "            for row in rows:\n",
    "                row_dict = {\n",
    "                    \"id\": row.id if row.id is not None else None,\n",
    "                    \"chunk_content\": row.chunk_content if row.chunk_content is not None else None,\n",
    "                    \"summary\": row.summary if row.summary is not None else None,\n",
    "                    \"keywords_combined\": row.keywords_combined if row.keywords_combined is not None else None,\n",
    "                    \"document_name\": row.document_name if row.document_name is not None else None,\n",
    "                    \"document_url\": row.document_url if row.document_url is not None else None,\n",
    "                    \"table_name\": row.table_name if row.table_name is not None else None,\n",
    "                    \"database_name\": row.database_name if row.database_name is not None else None,\n",
    "                    \"page_number\": row.page_number if row.page_number is not None else None,\n",
    "                    \"similarity\": row.similarity if row.similarity is not None else None\n",
    "                }\n",
    "                row_json = json.dumps(row_dict, indent=4)\n",
    "                print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "                print(row_json)\n",
    "                print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "                # Adding the type attribute based on conditions\n",
    "                if not row_dict[\"document_name\"] and row_dict[\"table_name\"]:\n",
    "                    row_dict[\"type\"] = \"structured\"\n",
    "                    structured_count += 1\n",
    "                elif not row_dict[\"table_name\"] and row_dict[\"document_name\"]:\n",
    "                    row_dict[\"type\"] = \"unstructured\"\n",
    "                    unstructured_count += 1\n",
    "        else:\n",
    "            print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "            print(\"No similar embeddings found.\")\n",
    "            print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "\n",
    "            joined_keywords =get_joined_keywords(keyword, seperator='|')\n",
    "            print(f\"joined_keywords: {joined_keywords}\")\n",
    "\n",
    "            rows = retriever_fn(action_paragraph_embedding_str, joined_words=joined_keywords, roles=roles)\n",
    "\n",
    "            unstructured_count = 0\n",
    "            structured_count = 0\n",
    "            for row in rows:\n",
    "                row_dict = {\n",
    "                    \"id\": row.id if row.id is not None else None,\n",
    "                    \"chunk_content\": row.chunk_content if row.chunk_content is not None else None,\n",
    "                    \"summary\": row.summary if row.summary is not None else None,\n",
    "                    \"keywords_combined\": row.keywords_combined if row.keywords_combined is not None else None,\n",
    "                    \"document_name\": row.document_name if row.document_name is not None else None,\n",
    "                    \"document_url\": row.document_url if row.document_url is not None else None,\n",
    "                    \"table_name\": row.table_name if row.table_name is not None else None,\n",
    "                    \"database_name\": row.database_name if row.database_name is not None else None,\n",
    "                    \"page_number\": row.page_number if row.page_number is not None else None,\n",
    "                    \"similarity\": row.similarity if row.similarity is not None else None\n",
    "                }\n",
    "                row_json = json.dumps(row_dict, indent=4)\n",
    "                print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "                print(row_json)\n",
    "                print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "                # Adding the type attribute based on conditions\n",
    "                if not row_dict[\"document_name\"] and row_dict[\"table_name\"]:\n",
    "                    row_dict[\"type\"] = \"structured\"\n",
    "                    structured_count += 1\n",
    "                elif not row_dict[\"table_name\"] and row_dict[\"document_name\"]:\n",
    "                    row_dict[\"type\"] = \"unstructured\"\n",
    "                    unstructured_count += 1\n",
    "\n",
    "        if unstructured_count > structured_count:\n",
    "            types[i]=\"unstructured\"\n",
    "        else:\n",
    "            types[i]=\"structured\"\n",
    "\n",
    "    return actions, keywords, informations, types \n",
    "    \n",
    "\n",
    "def get_last_action_answer(json_input, key):\n",
    "    try:\n",
    "        # Validate input types\n",
    "        if not isinstance(json_input, dict):\n",
    "            raise ValueError(\"json_input must be a dictionary.\")\n",
    "        if not isinstance(key, str):\n",
    "            raise ValueError(\"key must be a string.\")\n",
    "            \n",
    "        # Check if the key exists in the json_input\n",
    "        if key not in json_input:\n",
    "            return \"No Relevant Keys\"\n",
    "        \n",
    "        # Extract the relevant section of the JSON\n",
    "        relevant_data = json_input.get(key)\n",
    "\n",
    "        # Check for the presence of action_list\n",
    "        action_list_section = relevant_data[1]\n",
    "        if \"action_list\" not in action_list_section:\n",
    "            return \"No Relevant Keys\"\n",
    "            \n",
    "        # Now check if \"actions\" exists in the action_list\n",
    "        action_list = action_list_section[\"action_list\"].get(\"actions\", {})\n",
    "            \n",
    "        # Ensure there are actions\n",
    "        if not action_list:\n",
    "            return \"No Relevant Keys\"\n",
    "            \n",
    "        # Find the last action and its answer\n",
    "        last_action_key = sorted(action_list.keys())[-1]\n",
    "        last_action = action_list[last_action_key]\n",
    "            \n",
    "        # Retrieve the answer\n",
    "        answer = last_action.get(\"answer\", \"No Answer Found\")\n",
    "            \n",
    "        return answer\n",
    "\n",
    "    except ValueError as ve:\n",
    "        print(f\"ValueError: {str(ve)}\")\n",
    "        return \"No Relevant Keys\"\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")\n",
    "        return \"No Relevant Keys\"\n",
    "    \n",
    "def get_last_action_sql(json_input, key):\n",
    "    try:\n",
    "        # Validate input types\n",
    "        if not isinstance(json_input, dict):\n",
    "            raise ValueError(\"json_input must be a dictionary.\")\n",
    "        if not isinstance(key, str):\n",
    "            raise ValueError(\"key must be a string.\")\n",
    "            \n",
    "        # Check if the key exists in the json_input\n",
    "        if key not in json_input:\n",
    "            return \"\"\n",
    "        \n",
    "        # Extract the relevant section of the JSON\n",
    "        relevant_data = json_input.get(key)\n",
    "\n",
    "        # Check for the presence of action_list\n",
    "        action_list_section = relevant_data[1]\n",
    "        if \"action_list\" not in action_list_section:\n",
    "            return \"\"\n",
    "            \n",
    "        # Now check if \"actions\" exists in the action_list\n",
    "        action_list = action_list_section[\"action_list\"].get(\"actions\", {})\n",
    "            \n",
    "        # Ensure there are actions\n",
    "        if not action_list:\n",
    "            return \"\"\n",
    "            \n",
    "        # Find the last action and its SQL\n",
    "        last_action_key = sorted(action_list.keys())[-1]\n",
    "        last_action = action_list[last_action_key]\n",
    "            \n",
    "        # Retrieve the SQL query\n",
    "        sql_query = last_action.get(\"sql\", \"\")\n",
    "            \n",
    "        return sql_query\n",
    "\n",
    "    except ValueError as ve:\n",
    "        print(f\"ValueError: {str(ve)}\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### concurrent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "\n",
    "\n",
    "def get_response(\n",
    "    question,\n",
    "    chat_history,\n",
    "    retriever_fn=find_similar_embeddings_by_keyword_and_chunk_embedding,\n",
    "    question_analysis=[],\n",
    "    rows=[],\n",
    "    role_name=['tesco_org'],\n",
    "    is_keyword=True,\n",
    "    user_sessions=None,\n",
    "    run_id=None,\n",
    "):\n",
    "    columns = [\n",
    "        \"question\",\n",
    "        \"keywords\",\n",
    "        \"actions\",\n",
    "        \"information\",\n",
    "        \"entities\",\n",
    "        \"role\",\n",
    "        \"Similar_chunks_Question_AND\",\n",
    "        \"Similar_chunks_Question_OR\",\n",
    "        \"structured_count\",\n",
    "        \"unstructured_count\",\n",
    "        \"query_type\",\n",
    "        \"Current_Action\",\n",
    "        \"Action_chunks\",\n",
    "        \"Action_Similarity_Score\",\n",
    "        \"action_structured_count\",\n",
    "        \"action_unstructured_count\",\n",
    "        \"action_highest_structured_similarity\",\n",
    "        \"action_highest_unstructured_similarity\",\n",
    "        \"Action_query_sql\",\n",
    "        \"Action_query_resultset\",\n",
    "        \"Action_answer\",\n",
    "        \"Action_Quality\",\n",
    "        \"Action_query_sql_with_similarity\",\n",
    "        \"Action_query_resultset_with_similarity\",\n",
    "        \"Action_answer_with_similarity\",\n",
    "        \"final_answer\",\n",
    "    ]\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "\n",
    "    if not question_analysis and len(question_analysis)<1:\n",
    "\n",
    "        print(\"++++++++++++++++++++++++ inside ++++++++++++++++++++++\")\n",
    "        row=rows[0]\n",
    "        chat_history['original_actions']= row.get('original_actions',{})\n",
    "\n",
    "        if retriever_fn == find_similar_embeddings_by_keyword_and_chunk_embedding:\n",
    "            new_actions = row.get('keyword_chunk_emb_combined_actions',{\"types\": [],\"actions\": [],\"keywords\": [[]],\"informations\": []}).get(\"actions\",[])\n",
    "            new_keywords= row.get('keyword_chunk_emb_combined_actions',{\"types\": [],\"actions\": [],\"keywords\": [[]],\"informations\": []}).get(\"keywords\",[])\n",
    "            new_types= row.get('keyword_chunk_emb_combined_actions',{\"types\": [],\"actions\": [],\"keywords\": [[]],\"informations\": []}).get(\"types\",[])\n",
    "            new_information= row.get('keyword_chunk_emb_combined_actions',{\"types\": [],\"actions\": [],\"keywords\": [[]],\"informations\": []}).get(\"informations\",[])\n",
    "            \n",
    "            chat_history['keyword_chunk_emb_modified_actions']= row.get('keyword_chunk_emb_modified_actions',{})\n",
    "            chat_history['keyword_chunk_emb_combined_actions']= row.get('keyword_chunk_emb_combined_actions',{})\n",
    "\n",
    "        elif retriever_fn == find_similar_embeddings_by_entities_and_chunk_embedding:\n",
    "            new_actions = row.get('entity_chunk_emb_combined_actions',{\"types\": [],\"actions\": [],\"keywords\": [[]],\"informations\": []}).get(\"actions\",[])\n",
    "            new_keywords= row.get('entity_chunk_emb_combined_actions',{\"types\": [],\"actions\": [],\"keywords\": [[]],\"informations\": []}).get(\"keywords\",[])\n",
    "            new_types= row.get('entity_chunk_emb_combined_actions',{\"types\": [],\"actions\": [],\"keywords\": [[]],\"informations\": []}).get(\"types\",[])\n",
    "            new_information= row.get('entity_chunk_emb_combined_actions',{\"types\": [],\"actions\": [],\"keywords\": [[]],\"informations\": []}).get(\"informations\",[])\n",
    "            \n",
    "            chat_history['entity_chunk_emb_modified_actions']= row.get('entity_chunk_emb_modified_actions',{})\n",
    "            chat_history['entity_chunk_emb_combined_actions']= row.get('entity_chunk_emb_combined_actions',{})\n",
    "\n",
    "        elif retriever_fn == find_similar_embeddings_by_keyword_and_questions_embedding:\n",
    "            new_actions = row.get('keyword_question_emb_combined_actions',{\"types\": [],\"actions\": [],\"keywords\": [[]],\"informations\": []}).get(\"actions\",[])\n",
    "            new_keywords= row.get('keyword_question_emb_combined_actions',{\"types\": [],\"actions\": [],\"keywords\": [[]],\"informations\": []}).get(\"keywords\",[])\n",
    "            new_types= row.get('keyword_question_emb_combined_actions',{\"types\": [],\"actions\": [],\"keywords\": [[]],\"informations\": []}).get(\"types\",[])\n",
    "            new_information= row.get('keyword_question_emb_combined_actions',{\"types\": [],\"actions\": [],\"keywords\": [[]],\"informations\": []}).get(\"informations\",[])\n",
    "\n",
    "            chat_history['keyword_question_emb_modified_actions']= row.get('keyword_question_emb_modified_actions',{})\n",
    "            chat_history['keyword_question_emb_combined_actions']= row.get('keyword_question_emb_combined_actions',{})\n",
    "\n",
    "\n",
    "\n",
    "    else:\n",
    "        # result, baseline_list = get_actions(question, question_analysis, keywords=keywords_or_entities, actions=actions, retriever_fn=retriever_fn)\n",
    "        original_actions, original_keywords, original_informations, original_types = extract_analysis_details(question_analysis)\n",
    "        originl_types = {\n",
    "        \"actions\": original_actions,\n",
    "        \"keywords\": original_keywords,\n",
    "        \"informations\": original_informations,\n",
    "        \"types\": original_types\n",
    "        } \n",
    "\n",
    "        actions, keywords, informations, types = modify_type_for_each_action(\n",
    "            question,\n",
    "            question_analysis,\n",
    "            retriever_fn=retriever_fn,\n",
    "            role_name=role_name,\n",
    "        )\n",
    "\n",
    "        modified_types = {\n",
    "        \"actions\": actions,\n",
    "        \"keywords\": keywords,\n",
    "        \"informations\": informations,\n",
    "        \"types\": types\n",
    "        }\n",
    "        modified_types_json = json.dumps(modified_types, indent=4)\n",
    "        print(\"After Modifying action types: \", modified_types_json)\n",
    "\n",
    "        # Combine consecutive actions, keywords, and types if they share the same type\n",
    "        new_actions, new_keywords, new_types, new_information = combine_consecutive_actions(actions, keywords, types, informations)\n",
    "\n",
    "        combined_modified_types = {\n",
    "        \"actions\": new_actions,\n",
    "        \"keywords\": new_keywords,\n",
    "        \"informations\": new_information,\n",
    "        \"types\": new_types\n",
    "        }\n",
    "        combined_modified_types_json = json.dumps(combined_modified_types, indent=4)\n",
    "        print(\"After combining consecutive similar action types: \",combined_modified_types_json)\n",
    "\n",
    "        chat_history['original_actions']= originl_types\n",
    "        \n",
    "        if retriever_fn == find_similar_embeddings_by_keyword_and_chunk_embedding:\n",
    "            chat_history['keyword_chunk_emb_modified_actions']= modified_types\n",
    "            chat_history['keyword_chunk_emb_combined_actions']= combined_modified_types \n",
    "        elif retriever_fn == find_similar_embeddings_by_entities_and_chunk_embedding:\n",
    "            chat_history['entity_chunk_emb_modified_actions']= modified_types\n",
    "            chat_history['entity_chunk_emb_combined_actions']= combined_modified_types\n",
    "        elif retriever_fn == find_similar_embeddings_by_keyword_and_questions_embedding:\n",
    "            chat_history['keyword_question_emb_modified_actions']= modified_types\n",
    "            chat_history['keyword_question_emb_combined_actions']= combined_modified_types\n",
    "\n",
    "    baseline_list = []\n",
    "    baseline_list.append(question)\n",
    "    baseline_list.append(new_keywords)\n",
    "    baseline_list.append(new_actions)\n",
    "    baseline_list.append(new_information)\n",
    "    baseline_list.append(\"\")\n",
    "    baseline_list.append(role_name)\n",
    "    baseline_list.append(\"\")\n",
    "    baseline_list.append(\"\")\n",
    "    baseline_list.append(\"\")\n",
    "    baseline_list.append(\"\")\n",
    "    baseline_list.append(\"\")\n",
    "\n",
    "    baseline = {}\n",
    "    baseline['question'] = question\n",
    "    baseline['question_analysis'] = question_analysis\n",
    "    baseline['original_actions'] = new_actions\n",
    "    baseline['original_keywords'] = new_keywords\n",
    "    baseline['modified_query_types'] = new_types\n",
    "    baseline['new_actions'] = new_actions\n",
    "    baseline['new_keywords'] = new_keywords\n",
    "    baseline['new_query_types'] = new_types\n",
    "    baseline['information'] = new_information\n",
    "    baseline['action_results'] = []\n",
    "    baseline['result_json']=None\n",
    "    baseline['answer']=None\n",
    "    baseline['answer_path']=None\n",
    "\n",
    "    # print(new_actions,new_keywords,new_types)\n",
    "\n",
    "\n",
    "    if is_keyword:\n",
    "        keywords_or_entities = new_keywords\n",
    "    else:\n",
    "        keywords_or_entities = []\n",
    "\n",
    "\n",
    "\n",
    "    final_response, all_baselines, action_list = get_action_response(\n",
    "        question=question,\n",
    "        actions=new_actions,\n",
    "        keywords=keywords_or_entities,\n",
    "        types=new_types,\n",
    "        baseline_list=baseline_list,\n",
    "        retriever_fn=retriever_fn,\n",
    "        role_name=role_name,\n",
    "        user_sessions=user_sessions,\n",
    "        run_id=run_id,\n",
    "    )\n",
    "\n",
    "    baseline['action_results']=all_baselines\n",
    "    baseline['answer']=final_response\n",
    "    baseline['answer_path']=retriever_fn\n",
    "\n",
    "    new_df = pd.DataFrame(all_baselines, columns=columns)\n",
    "    df = pd.concat([df, new_df], ignore_index=True)\n",
    "\n",
    "    # import os\n",
    "    # if not os.path.exists('baseline.csv'):\n",
    "    #     df = pd.DataFrame({key: [value] for key, value in baseline.items()})\n",
    "    #     df.to_csv('baseline.csv',index=False)\n",
    "    # else:\n",
    "    #     df1=pd.read_csv('baseline.csv')\n",
    "    #     df2=pd.DataFrame({key: [value] for key, value in baseline.items()})\n",
    "    #     result = pd.concat([df1, df2], axis=0)\n",
    "    #     result.to_csv('baseline.csv',index=False)\n",
    "\n",
    "    return {\"response\": final_response}, {\"action_list\": action_list}\n",
    "\n",
    "def run_concurrent_tasks(question, chat_history, role_name=[], user_sessions=None, run_id=None):\n",
    "    ## Role Hardcoded\n",
    "    role_name=['tesco_org']\n",
    "\n",
    "    # question_embedding = generate_openai_embeddings(question)\n",
    "    user_id=\"7d1d0479-f372-4a6d-83b8-485a8368ba4a\"\n",
    "    is_favourite_question, rows = False, []\n",
    "\n",
    "    is_keyword_values = {\n",
    "        \"keyword_chunk_emb\": True,\n",
    "        \"entity_chunk_emb\": False,\n",
    "        \"keyword_question_emb\": True,\n",
    "    }\n",
    "\n",
    "    question_analysis = []\n",
    "\n",
    "    if is_favourite_question is not True:\n",
    "        prompt = KEYWORDS_ACTIONS_INFO_TYPE_EXTRACTOR.format(question=question)\n",
    "        # Get linked actions\n",
    "\n",
    "        question_analysis = get_keywords_linkedq_typeinfo_from_question(\n",
    "        question, prompt=prompt\n",
    "        )\n",
    "    \n",
    "    print(\"+++++++++++ question analysis +++++++++++++++++\", question_analysis)\n",
    "\n",
    "    result_dict = {}\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = {\n",
    "            executor.submit(\n",
    "                get_response,\n",
    "                question,\n",
    "                chat_history,\n",
    "                find_similar_embeddings_by_keyword_and_chunk_embedding,\n",
    "                question_analysis,\n",
    "                rows,\n",
    "                role_name,\n",
    "                is_keyword_values[\"keyword_chunk_emb\"],\n",
    "                user_sessions,\n",
    "                run_id=run_id,\n",
    "            ): \"keyword_chunk_emb\",\n",
    "            executor.submit(\n",
    "                get_response,\n",
    "                question,\n",
    "                chat_history,\n",
    "                find_similar_embeddings_by_entities_and_chunk_embedding,\n",
    "                question_analysis,\n",
    "                rows,\n",
    "                role_name,\n",
    "                is_keyword_values[\"entity_chunk_emb\"],\n",
    "                user_sessions,\n",
    "                run_id=run_id,\n",
    "            ): \"entity_chunk_emb\",\n",
    "            executor.submit(\n",
    "                get_response,\n",
    "                question,\n",
    "                chat_history,\n",
    "                find_similar_embeddings_by_keyword_and_questions_embedding,\n",
    "                question_analysis,\n",
    "                rows,\n",
    "                role_name,\n",
    "                is_keyword_values[\"keyword_question_emb\"],\n",
    "                user_sessions,\n",
    "                run_id=run_id,\n",
    "            ): \"keyword_question_emb\",\n",
    "        }\n",
    "\n",
    "        tasks_completed, _ = concurrent.futures.wait(\n",
    "            futures, return_when=concurrent.futures.ALL_COMPLETED\n",
    "        )\n",
    "\n",
    "        for future in tasks_completed:\n",
    "            name = futures[future]\n",
    "            try:\n",
    "                answer = future.result()\n",
    "                result_dict[name] = answer\n",
    "            except Exception as exc:\n",
    "                traceback.print_exc()\n",
    "                print(f\"{name} generated an exception: {exc}\")\n",
    "                traceback.print_exc()\n",
    "\n",
    "    print(\"+++ result dict +++++\",result_dict)\n",
    "\n",
    "    return result_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions=[     \n",
    "    # \"What is the net content of sku ID 220314146?\",\n",
    "    #             \"List all promotions and their corresponding discount percentages?\",\n",
    "    #             \"Find all unique events related to promotions?\",\n",
    "    #             \"How many employees are present in Mokate?\",\n",
    "                    \"List all unique promotions\",\n",
    "                # \"Brief me about Mokate\",  \n",
    "                # \"Brief me about Pepe\",\n",
    "                # \"Give me the detailed product information, including nutritional information for sku ID 216357626.\",\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "def generate_answer(questions):\n",
    "    results = []\n",
    "    for question in questions:\n",
    "        chat_history = {\n",
    "            \"run_id\" : str(uuid.uuid4()),\n",
    "            \"session_id\": str(uuid.uuid4()),\n",
    "            \"user_id\": \"50deb7bb-9f71-4ec8-8fb7-7043ed5d7ab5\",\n",
    "            \"question\": question,\n",
    "            # \"question_embedding\":[],\n",
    "            \"original_actions\":{},\n",
    "            \"keyword_chunk_emb_modified_actions\":{},\n",
    "            \"keyword_chunk_emb_combined_actions\":{},\n",
    "            \"keyword_chunk_emb_response\":{},\n",
    "            \"entity_chunk_emb_modified_actions\":{},\n",
    "            \"entity_chunk_emb_combined_actions\":{},\n",
    "            \"entity_chunk_emb_response\":{},\n",
    "            \"keyword_question_emb_modified_actions\":{},\n",
    "            \"keyword_question_emb_combined_actions\":{},\n",
    "            \"keyword_question_emb_response\":{},\n",
    "            \"final_answer\": {},\n",
    "            \"is_favorite\": True,\n",
    "            \"feedback\": True\n",
    "        }\n",
    "        \n",
    "        # Start timing\n",
    "        start_time = time.time()\n",
    "\n",
    "        result_dict=run_concurrent_tasks(question=question, chat_history=chat_history)\n",
    "\n",
    "        responses = {}\n",
    "        final_dict = {\n",
    "            \"keyword_chunk_emb\":{},\n",
    "            \"entity_chunk_emb\":{},\n",
    "            \"keyword_question_emb\":{},\n",
    "            \"best_answer\":{\"answer\":\"No Answer Found\"}\n",
    "        }\n",
    "\n",
    "        keys = [\"keyword_chunk_emb\", \"entity_chunk_emb\", \"keyword_question_emb\"]\n",
    "        for key in keys:\n",
    "            responses[key] = result_dict.get(\n",
    "                key,\n",
    "                ({\"response\": {\"answer\": \"No answer found!!\"}}, {\"action_list\": {}}),\n",
    "            )[0][\"response\"][\"answer\"]\n",
    "            final_dict[key] = result_dict.get(\n",
    "                key,\n",
    "                ({\"response\": {\"answer\": \"No Answer Found!!\"}}, {\"action_list\": {}}),\n",
    "            )[1][\"action_list\"]\n",
    "\n",
    "        best_answer,final_answer_selector_prompt_str, final_answer_selector_prompt_str_v2 = get_final_answer(answer_dict=final_dict)\n",
    "\n",
    "        print(best_answer)\n",
    "\n",
    "        if best_answer['key'] in keys:\n",
    "            final_dict[\"best_answer\"] = {\n",
    "            \"answer\": get_last_action_answer(result_dict,best_answer[\"key\"]),\n",
    "            best_answer[\"key\"]: final_dict[best_answer[\"key\"]],\n",
    "            }     \n",
    "\n",
    "        # Stop timing\n",
    "        end_time = time.time()\n",
    "        chat_history['keyword_chunk_emb_response']=final_dict['keyword_chunk_emb']\n",
    "        chat_history['entity_chunk_emb_response']=final_dict['entity_chunk_emb']\n",
    "        chat_history['keyword_question_emb_response']=final_dict['keyword_question_emb']\n",
    "        chat_history['final_answer']=final_dict['best_answer']\n",
    "\n",
    "        # store_chat_history(chat_history)\n",
    "\n",
    "        # Calculate the elapsed time in seconds\n",
    "        execution_time = end_time - start_time\n",
    "\n",
    "        # Append the question and its final_dict to the results list\n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"time\":execution_time,\n",
    "            \"Original Prompts Given to LLM\":final_answer_selector_prompt_str,\n",
    "            \"Prompt afrer simplified\": final_answer_selector_prompt_str_v2,\n",
    "            \"sql\": get_last_action_sql(result_dict,best_answer[\"key\"]),\n",
    "            \"best Answer\":final_dict[\"best_answer\"][\"answer\"],\n",
    "            \"Retriever_best\":best_answer[\"key\"],\n",
    "            \"response\":json.dumps(final_dict, indent=4)\n",
    "        })\n",
    "\n",
    "        print(json.dumps(chat_history, indent=4))\n",
    "\n",
    "        chat_history['final_answer']=final_dict['best_answer']\n",
    "\n",
    "generate_answer(questions)\n",
    "# Convert the results list to a DataFrame\n",
    "# df = pd.DataFrame(results)\n",
    "# df.to_csv(\"Answers.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5=get_top_matched_glossary(\"50deb7bb-9f71-4ec8-8fb7-7043ed5d7ab5\", generate_question_embeddings(questions[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_question=generate_modified_question_with_glossary(question=questions[0], top_matches= top_5).get(\"modified_question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = KEYWORDS_ACTIONS_INFO_TYPE_EXTRACTOR.format(question=modified_question)\n",
    "        # Get linked actions\n",
    "\n",
    "question_analysis = get_keywords_linkedq_typeinfo_from_question(modified_question, prompt=prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
